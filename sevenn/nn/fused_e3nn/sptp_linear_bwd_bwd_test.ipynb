{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import e3nn\n",
    "from torch.utils.cpp_extension import load\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import cuequivariance as cue\n",
    "import cuequivariance_torch as cuet\n",
    "import random\n",
    "from typing import List\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home2/lsy/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home2/lsy/.cache/torch_extensions/py311_cu124/sptp_linear_bwd_bwd/build.ninja...\n",
      "Building extension module sptp_linear_bwd_bwd...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module sptp_linear_bwd_bwd...\n"
     ]
    }
   ],
   "source": [
    "os.environ['TORCH_CUDA_ARCH_LIST'] = \"8.0\"\n",
    "sptp_bwd_bwd = load(name='sptp_linear_bwd_bwd', sources=['/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/sptp_linear_bwd_bwd.cpp', \n",
    "                                  '/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/bwd_bwd_sptp_linear_v2_shared.cu',\n",
    "                                  ], \n",
    "                                  extra_cuda_cflags=[\"-lineinfo\"], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_Irreps(mul, i_in):\n",
    "    dd = []\n",
    "    for ori_mul, ir in i_in:\n",
    "        dd.append((ori_mul*mul, (ir.l, ir.p)))\n",
    "    return e3nn.o3.Irreps(dd)\n",
    "def compare(a, b):\n",
    "    isclose = torch.isclose(a, b)\n",
    "    diff_pos = torch.argwhere(isclose == False)\n",
    "    anything_bad = False\n",
    "    for pos in diff_pos:\n",
    "        pos_t = [x for x in pos]\n",
    "        if(abs(a[pos_t] - b[pos_t]) > 1e-6):\n",
    "            anything_bad = True\n",
    "            print(pos)\n",
    "            print(a[pos_t] - b[pos_t] )\n",
    "    if(not anything_bad):\n",
    "        print(\"All Good\")\n",
    "            \n",
    "IR_IN1_IDX = 0\n",
    "IR_IN2_IDX = 1\n",
    "IR_OUT_IDX = 2\n",
    "INST_IDX = 3\n",
    "\n",
    "def load_nequip_config(h, l_max, layer_idx):\n",
    "    filename = f\"/home2/lsy/mdsim/nequip/benchmark_config/4_{h}_{l_max}_p_sc.txt\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        f_in = f.read().split(\"\\n\")\n",
    "\n",
    "    per_layer_dict = dict()\n",
    "    for l_idx, d in enumerate(f_in):\n",
    "        if(d == \"\") : continue\n",
    "        dd = json.loads(d)\n",
    "        per_layer_dict[l_idx] = dd\n",
    "    tp_list = per_layer_dict[layer_idx][\"tp\"]\n",
    "    i_in1 = e3nn.o3.Irreps(tp_list[IR_IN1_IDX])\n",
    "    i_in2 = e3nn.o3.Irreps(tp_list[IR_IN2_IDX])\n",
    "    i_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "    inst_tuple = [tuple(x) for x in tp_list[INST_IDX]]\n",
    "\n",
    "    return i_in1, i_in2, i_out, inst_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nequip_config_e3nn_cueq(h, l_max, layer_idx):\n",
    "    filename = f\"/home2/lsy/mdsim/nequip/benchmark_config/4_{h}_{l_max}_p_sc.txt\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        f_in = f.read().split(\"\\n\")\n",
    "\n",
    "    per_layer_dict = dict()\n",
    "    for l_idx, d in enumerate(f_in):\n",
    "        if(d == \"\") : continue\n",
    "        dd = json.loads(d)\n",
    "        per_layer_dict[l_idx] = dd\n",
    "    tp_list = per_layer_dict[layer_idx][\"tp\"]\n",
    "\n",
    "    ei_in1 = e3nn.o3.Irreps(tp_list[IR_IN1_IDX])\n",
    "    ei_in2 = e3nn.o3.Irreps(tp_list[IR_IN2_IDX])\n",
    "    ei_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "    inst_tuple = [tuple(x) for x in tp_list[INST_IDX]]\n",
    "\n",
    "\n",
    "    # changing mul for each ir.l\n",
    "    new_in1_list = []\n",
    "    new_out_list = []\n",
    "    changed_idx = [[],[]]\n",
    "    # mul_list = {}\n",
    "    mul_list = {0:64, 1:32}\n",
    "\n",
    "    for idx, (mul,ir) in enumerate(ei_in1):\n",
    "        if (ir.l in mul_list):\n",
    "            new_in1_list.append((mul_list[ir.l], ir))\n",
    "            for inst in inst_tuple:\n",
    "                if(idx == inst[0]):\n",
    "                    changed_idx[0].append(inst[2])\n",
    "                    changed_idx[1].append(mul_list[ir.l])\n",
    "        else:\n",
    "            new_in1_list.append((mul, ir))\n",
    "\n",
    "    for idx, (mul,ir) in enumerate(ei_out):\n",
    "        if (idx in changed_idx[0]):\n",
    "            new_out_list.append((changed_idx[1][changed_idx[0].index(idx)], ir))\n",
    "        else:\n",
    "            new_out_list.append((mul, ir))\n",
    "\n",
    "    ei_in1 = e3nn.o3.Irreps(new_in1_list)\n",
    "    ei_out = e3nn.o3.Irreps(new_out_list)\n",
    "\n",
    "    ci_in1 = cue.Irreps(\"O3\", str(ei_in1))\n",
    "    ci_in2 = cue.Irreps(\"O3\", tp_list[IR_IN2_IDX])\n",
    "    ci_out = cue.Irreps(\"O3\", str(ei_out))\n",
    "\n",
    "\n",
    "    return [ei_in1,ei_in2,ei_out,inst_tuple] , [ci_in1,ci_in2,ci_out,inst_tuple]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda_list(*arg, input_dtype = torch.float32):\n",
    "    return_list = []\n",
    "    for item in arg:\n",
    "        if(type(item) == torch.Tensor):\n",
    "            return_list.append(item.to(device=\"cuda\"))\n",
    "        else:\n",
    "            return_list.append(torch.tensor(item,device=\"cuda\", dtype=input_dtype))\n",
    "    return return_list\n",
    "\n",
    "def to_cuda_dict(strname_list, *arg):\n",
    "    return_dict = {}\n",
    "    for item,name in zip(arg,strname_list):\n",
    "        if(type(item) == torch.Tensor):\n",
    "            return_dict[name] = item.to(\"cuda\")\n",
    "        else:\n",
    "            return_dict[name] = torch.tensor(item,device=\"cuda\")\n",
    "    return return_dict\n",
    "\n",
    "def cumsum_list(s, np1 = True):\n",
    "    new_s = []\n",
    "    current = 0\n",
    "    for e in s:\n",
    "        new_s.append(current)\n",
    "        current += e\n",
    "    if(np1):\n",
    "        new_s.append(current)\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "h = 32\n",
    "l_max = 1\n",
    "layer_idx = 3\n",
    "batch_size = 4096\n",
    "# i_in1, i_in2, i_out, inst_tuple = load_nequip_config(h,l_max,layer_idx)\n",
    "# i_in2 = mul_Irreps(3, i_in2)\n",
    "e3nn_config, cueq_config = load_nequip_config_e3nn_cueq(h,l_max,layer_idx)\n",
    "i_in1, i_in2, i_out, inst_tuple = e3nn_config\n",
    "\n",
    "# not really needed for v=1 \n",
    "uvuv_tp = e3nn.o3.FullTensorProduct(i_in1,i_in2, filter_ir_out=i_out)\n",
    "uvuv_i_out = uvuv_tp.irreps_out\n",
    "\n",
    "# split_size = []\n",
    "# reshape_size = []\n",
    "# for inst in uvuv_tp.instructions:\n",
    "#     split_size.append(uvuv_i_out[inst.i_out].dim)\n",
    "#     reshape_size.append([inst.path_shape[0],inst.path_shape[1],uvuv_i_out[inst.i_out][1].dim])\n",
    "# weight_mul = e3nn.o3.experimental.FullTensorProduct_uvu_weight_only(i_in1, i_in2, split_size, reshape_size, filter_ir_out=i_out, irrep_normalization=None, regroup_output=False)\n",
    "# uvw\n",
    "# i_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "# tp = e3nn.o3.FullyConnectedTensorProduct(i_in1,i_in2,i_out,shared_weights=False, internal_weights=False)\n",
    "\n",
    "# # uvu\n",
    "tp = e3nn.o3.TensorProduct(i_in1,i_in2,i_out,inst_tuple,shared_weights=False, internal_weights=False,)#path_normalization=\"none\", normalization=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_uvu = e3nn.o3.experimental.FullTensorProduct_grad_uvu(i_in1,i_in2, filter_ir_out=i_out, irrep_normalization=\"component\", regroup_output=False)\n",
    "bwd_uvu = e3nn.o3.experimental.FullTensorProduct_bwd_uvu(i_in1,i_in2, filter_ir_out=i_out, irrep_normalization=\"component\", regroup_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute density tensor(0.2909)\n"
     ]
    }
   ],
   "source": [
    "# full tp -> linear\n",
    "# i_out = full_tp.irreps_out\n",
    "unique_cg = []\n",
    "unique_cg_mat = {}\n",
    "nnz_cg_cnt = 0\n",
    "all_cg_cnt = 0\n",
    "cg_dummy = torch.zeros(i_in1.dim, i_in2.dim, uvuv_i_out.dim)\n",
    "cg_dummy_coverage = torch.zeros(i_in1.dim, i_in2.dim, uvuv_i_out.dim)\n",
    "for inst in uvuv_tp.instructions:\n",
    "    i = inst.i_in1\n",
    "    j = inst.i_in2\n",
    "    k = inst.i_out\n",
    "\n",
    "    mul_in1, ir_in1 = i_in1[i]\n",
    "    mul_in2, ir_in2 = i_in2[j]\n",
    "    mul_out, ir_out = uvuv_i_out[k]\n",
    "\n",
    "    cg = e3nn.o3.wigner_3j(ir_in1.l, ir_in2.l, ir_out.l)\n",
    "    unique_cg += list(cg.unique())\n",
    "    all_cg_cnt+= cg.numel()\n",
    "    nnz_cg_cnt += cg.count_nonzero()\n",
    "\n",
    "    partial_mat_cg = torch.zeros(i_in1[i].dim, i_in2[j].dim, uvuv_i_out[k].dim)\n",
    "    # print(cg)\n",
    "    unique_cg_mat[f\"{ir_in1.l}_{ir_in2.l}_{ir_out.l}\"] = cg\n",
    "    \n",
    "    ## uvuv\n",
    "    for u,v in itertools.product(range(mul_in1), range(mul_in2)):\n",
    "        partial_mat_cg [u*ir_in1.dim:(u+1)*ir_in1.dim,\n",
    "        v*ir_in2.dim:(v+1)*ir_in2.dim,\n",
    "        (u*mul_in2+v)*ir_out.dim:(u*mul_in2+v+1)*ir_out.dim] = cg \n",
    "\n",
    "    cg_dummy[i_in1.slices()[i], i_in2.slices()[j], uvuv_i_out.slices()[k]] = partial_mat_cg\n",
    "    cg_dummy_coverage[i_in1.slices()[i], i_in2.slices()[j], uvuv_i_out.slices()[k]] = 1\n",
    "print(\"compute density\", nnz_cg_cnt / all_cg_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cg_val = list(set([x.item() for x in unique_cg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_inst_outorder = sorted(tp.instructions, key=lambda x : x.i_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already duplicate as num_path > num unique cg matrix\n",
    "per_path_fiber_start = [0]\n",
    "per_path_fiber_array = []\n",
    "\n",
    "for inst in tp_inst_outorder:\n",
    "    path_cg = unique_cg_mat[f\"{i_in1[inst.i_in1][1].l}_{i_in2[inst.i_in2][1].l}_{i_out[inst.i_out][1].l}\"]\n",
    "    for i,j,k in path_cg.nonzero():\n",
    "        cg_idx = unique_cg_val.index(path_cg[i,j,k])\n",
    "        per_path_fiber_array.append([i.item(),j.item(),k.item(),cg_idx])\n",
    "    per_path_fiber_start.append(len(per_path_fiber_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_in1_ir_pathinfo = {}\n",
    "for inst in tp_inst_outorder:\n",
    "    if(inst.i_in1 not in per_in1_ir_pathinfo):\n",
    "        per_in1_ir_pathinfo[inst.i_in1] = []\n",
    "    per_in1_ir_pathinfo[inst.i_in1].append([inst.i_out, inst.i_in2, inst.path_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_uv_pair = []\n",
    "weight_uv_pair_sorted_chunk = []\n",
    "out_order = []\n",
    "current = 0\n",
    "\n",
    "for inst in tp.instructions:\n",
    "    weight_uv_pair.append((i_in1[inst.i_in1][0], i_in2[inst.i_in2][0] ))\n",
    "    out_order.append(inst.i_out)\n",
    "for u,v in weight_uv_pair:\n",
    "    weight_uv_pair_sorted_chunk.append(slice(current,current+u*v))\n",
    "    current+=u*v\n",
    "out2weight_order = torch.tensor(out_order).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARPSIZE = 32\n",
    "in1_idxing = [0]\n",
    "in1_ival = []\n",
    "in1_related_path_idx = [0]\n",
    "\n",
    "path_array1 = []\n",
    "path_array2 = []\n",
    "path_weight = []\n",
    "per_path_weight_pos = []\n",
    "\n",
    "per_upath_fiber_start = []\n",
    "per_upath_fiber_array = []\n",
    "\n",
    "in1_slices = i_in1.slices()\n",
    "in2_slices = i_in2.slices()\n",
    "out_slices = i_out.slices()\n",
    "\n",
    "for in1_ir_idx, (mul,ir) in enumerate(i_in1):\n",
    "    assert (mul%WARPSIZE ==0)\n",
    "    in1_idx_start = in1_slices[in1_ir_idx].start\n",
    "    i_val = ir.dim\n",
    "    \n",
    "    if mul >= WARPSIZE:\n",
    "        for i in range(mul//WARPSIZE):\n",
    "            in1_idxing.append(in1_idx_start + WARPSIZE*i_val*(i+1))\n",
    "            in1_ival.append(i_val)\n",
    "            in1_related_path_idx.append(in1_related_path_idx[-1] + len(per_in1_ir_pathinfo[in1_ir_idx]))\n",
    "            \n",
    "            dummy_list = []\n",
    "            dummy_list2 = []\n",
    "            # Bug? TODO:\n",
    "            for out_ir_idx, in2_ir_idx, pw in per_in1_ir_pathinfo[in1_ir_idx]:\n",
    "                # should be in order\n",
    "                fiber_start = per_path_fiber_start[out_ir_idx]\n",
    "                fiber_end = per_path_fiber_start[out_ir_idx+1]\n",
    "                \n",
    "                upath_fiber_start = len(per_upath_fiber_array)\n",
    "                upath_fiber_end = upath_fiber_start + fiber_end - fiber_start\n",
    "\n",
    "                per_upath_fiber_start.append([upath_fiber_start, upath_fiber_end])\n",
    "                # print(fiber_array_orignal[1:4])\n",
    "                # print(fiber_start,fiber_end)\n",
    "                # print(fiber_array_orignal[fiber_start:fiber_end])\n",
    "                \n",
    "                per_upath_fiber_array += per_path_fiber_array[fiber_start:fiber_end]\n",
    "\n",
    "                dummy_list.append([out_slices[out_ir_idx].start + WARPSIZE*i_out[out_ir_idx].ir.dim * i,\n",
    "                                   out_slices[out_ir_idx].start + WARPSIZE*i_out[out_ir_idx].ir.dim * (i+1)\n",
    "                                   ])\n",
    "                dummy_list2.append([i_out[out_ir_idx].ir.dim,\n",
    "                                    in2_slices[in2_ir_idx].start,\n",
    "                                    i_in2[in2_ir_idx].ir.dim,\n",
    "                                    in2_slices[in2_ir_idx].stop])\n",
    "                path_weight.append(pw)\n",
    "                \n",
    "                # TODO:??\n",
    "                per_path_weight_pos.append(weight_uv_pair_sorted_chunk[out2weight_order[out_ir_idx]].start + WARPSIZE*i)\n",
    "\n",
    "            path_array1.append(dummy_list)\n",
    "            path_array2.append(dummy_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_in1_idxing = torch.tensor(in1_idxing, dtype=torch.int32, device=\"cuda\")\n",
    "t_in1_ival = torch.tensor(in1_ival, dtype=torch.int32, device=\"cuda\")\n",
    "t_in1_related_path_idx = torch.tensor(in1_related_path_idx, dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "t_path_array1 = torch.tensor(list(itertools.chain.from_iterable(path_array1)), dtype=torch.uint16, device=\"cuda\")\n",
    "t_path_array2 = torch.tensor(list(itertools.chain.from_iterable(path_array2)), dtype=torch.uint8, device=\"cuda\")\n",
    "t_path_weight = torch.tensor(path_weight, dtype=torch.float32, device=\"cuda\")\n",
    "t_per_path_weight_pos = torch.tensor(per_path_weight_pos, dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "t_per_upath_fiber_start = torch.tensor(per_upath_fiber_start, dtype=torch.uint16, device=\"cuda\")\n",
    "t_per_upath_fiber_array = torch.tensor(per_upath_fiber_array, dtype=torch.uint8, device=\"cuda\")\n",
    "\n",
    "t_unique_cg_val = torch.tensor(unique_cg_val, dtype=torch.float32, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = torch.rand(batch_size, i_in1.dim, device=\"cuda\", requires_grad=True)\n",
    "in2 = torch.rand(batch_size, i_in2.dim, device=\"cuda\", requires_grad=True)\n",
    "weight = torch.rand(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "# weight = torch.ones(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "in1.retain_grad()\n",
    "in2.retain_grad()\n",
    "weight.retain_grad()\n",
    "## weight is tp.instruction order not out order\n",
    "\n",
    "in1_c = in1.clone().detach()\n",
    "in2_c = in2.clone().detach()\n",
    "weight_c = weight.clone().detach()\n",
    "\n",
    "in1_c.requires_grad =True\n",
    "in2_c.requires_grad =True\n",
    "weight_c.requires_grad =True\n",
    "\n",
    "\n",
    "# retain_grad_macro([in1_gen, in2_gen, weight_gen, in1_c, in2_c, wei])\n",
    "tp = tp.cuda()\n",
    "grad_uvu = grad_uvu.cuda()\n",
    "bwd_uvu = bwd_uvu.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_exp = tp(in1,in2,weight)\n",
    "out_exp.retain_grad()\n",
    "# out_exp = grad_uvu(in1,in2,weight)\n",
    "# out_exp.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.nn.functional.gelu(out_exp).sum()\n",
    "y.retain_grad()\n",
    "\n",
    "f_in1, f_in2, f_weight = torch.autograd.grad(y, [in1,in2,weight], create_graph=True)\n",
    "\n",
    "# y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2272,  1.0290,  0.7366,  ...,  0.1001,  0.1176,  0.1239],\n",
       "        [ 0.3063,  0.4565,  0.3690,  ...,  0.1806,  0.4159,  0.3247],\n",
       "        [ 0.2719,  0.7008,  0.6446,  ..., -0.0450,  0.0619,  0.2024],\n",
       "        ...,\n",
       "        [ 0.2035,  0.5889,  0.3202,  ...,  0.1482,  0.0851,  0.0524],\n",
       "        [ 0.9547,  0.5618,  0.4476,  ...,  0.4878,  0.5539,  0.2700],\n",
       "        [ 1.0585,  0.4030,  0.5671,  ...,  0.4976,  0.5019,  0.6377]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"97pt\" height=\"105pt\"\n",
       " viewBox=\"0.00 0.00 97.00 105.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 101)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-101 93,-101 93,4 -4,4\"/>\n",
       "<!-- 140075527197648 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140075527197648</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"89,-30 0,-30 0,0 89,0 89,-30\"/>\n",
       "<text text-anchor=\"middle\" x=\"44.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\">f_in1</text>\n",
       "<text text-anchor=\"middle\" x=\"44.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (4096, 320)</text>\n",
       "</g>\n",
       "<!-- 140075673407312 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140075673407312</title>\n",
       "<polygon fill=\"#a2cd5a\" stroke=\"black\" points=\"89,-97 0,-97 0,-66 89,-66 89,-97\"/>\n",
       "<text text-anchor=\"middle\" x=\"44.5\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\"> (4096, 320)</text>\n",
       "</g>\n",
       "<!-- 140075673407312&#45;&gt;140075527197648 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140075673407312&#45;&gt;140075527197648</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M44.5,-65.86C44.5,-58.34 44.5,-48.96 44.5,-40.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"48,-40.13 44.5,-30.13 41,-40.13 48,-40.13\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f65e04744d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(f_in1, params={\"y\":y, \"in1\":in1, \"in2\":in2,\"weight\":weight, \"f_in1\":f_in1, \"f_in2\":f_in2, \"f_weight\":f_weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4196,  1.4742,  1.4455,  ...,  0.1817,  0.1963,  0.1579],\n",
       "        [ 0.5642,  0.7783,  0.6315,  ...,  0.3729,  0.6434,  0.5800],\n",
       "        [ 0.5101,  0.9792,  0.8448,  ..., -0.0714,  0.0506,  0.3608],\n",
       "        ...,\n",
       "        [ 0.3416,  1.0657,  0.6023,  ...,  0.1989,  0.1678,  0.1246],\n",
       "        [ 1.7725,  1.0855,  0.8315,  ...,  0.2323,  0.9901,  0.4128],\n",
       "        [ 1.9217,  0.6360,  0.9834,  ...,  0.6603,  0.5394,  0.7542]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0078,  0.0100,  0.0005,  ...,  0.0329,  0.0214, -0.0286],\n",
       "        [ 0.2418,  0.2786,  0.2790,  ...,  0.0006, -0.0356,  0.0444],\n",
       "        [ 0.0516,  0.2150,  0.2629,  ..., -0.1135,  0.1353,  0.2274],\n",
       "        ...,\n",
       "        [ 0.2369,  0.1028,  0.1386,  ...,  0.0089,  0.0003,  0.0157],\n",
       "        [ 0.0582,  0.0388,  0.1224,  ..., -0.0850, -0.1119, -0.1445],\n",
       "        [ 0.0764,  0.3005,  0.2009,  ..., -0.0264,  0.0587,  0.0074]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[84.0002, 41.8671, 41.6277, 40.9111],\n",
       "        [87.9291, 45.4150, 47.8192, 46.6050],\n",
       "        [80.1224, 40.9374, 41.5159, 43.6381],\n",
       "        ...,\n",
       "        [82.8046, 39.6457, 42.5469, 42.2487],\n",
       "        [78.1380, 40.3616, 42.4801, 43.2001],\n",
       "        [78.1946, 46.2624, 45.9646, 48.4586]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_in2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4196,  1.4742,  1.4455,  ...,  0.1817,  0.1963,  0.1579],\n",
      "        [ 0.5642,  0.7783,  0.6315,  ...,  0.3729,  0.6434,  0.5800],\n",
      "        [ 0.5101,  0.9792,  0.8448,  ..., -0.0714,  0.0506,  0.3608],\n",
      "        ...,\n",
      "        [ 0.3416,  1.0657,  0.6023,  ...,  0.1989,  0.1678,  0.1246],\n",
      "        [ 1.7725,  1.0855,  0.8315,  ...,  0.2323,  0.9901,  0.4128],\n",
      "        [ 1.9217,  0.6360,  0.9834,  ...,  0.6603,  0.5394,  0.7542]],\n",
      "       device='cuda:0')\n",
      "tensor([[84.0002, 41.8671, 41.6277, 40.9111],\n",
      "        [87.9291, 45.4150, 47.8192, 46.6050],\n",
      "        [80.1224, 40.9374, 41.5159, 43.6381],\n",
      "        ...,\n",
      "        [82.8046, 39.6457, 42.5469, 42.2487],\n",
      "        [78.1380, 40.3616, 42.4801, 43.2001],\n",
      "        [78.1946, 46.2624, 45.9646, 48.4586]], device='cuda:0')\n",
      "tensor([[ 0.0078,  0.0100,  0.0005,  ...,  0.0329,  0.0214, -0.0286],\n",
      "        [ 0.2418,  0.2786,  0.2790,  ...,  0.0006, -0.0356,  0.0444],\n",
      "        [ 0.0516,  0.2150,  0.2629,  ..., -0.1135,  0.1353,  0.2274],\n",
      "        ...,\n",
      "        [ 0.2369,  0.1028,  0.1386,  ...,  0.0089,  0.0003,  0.0157],\n",
      "        [ 0.0582,  0.0388,  0.1224,  ..., -0.0850, -0.1119, -0.1445],\n",
      "        [ 0.0764,  0.3005,  0.2009,  ..., -0.0264,  0.0587,  0.0074]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(in1.grad)\n",
    "print(in2.grad)\n",
    "print(weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = out_exp.grad.detach().clone()\n",
    "new_out.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_c_grad, in2_c_grad, w_c_grad = bwd_uvu(in1_c, in2_c,new_out , weight_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_c_grad.retain_grad()\n",
    "in2_c_grad.retain_grad()\n",
    "w_c_grad.retain_grad()\n",
    "w_c_grad_gelu = torch.nn.functional.gelu(w_c_grad)\n",
    "in1_c_grad_gelu = torch.nn.functional.gelu(in1_c_grad)\n",
    "in2_c_grad_gelu = torch.nn.functional.gelu(in2_c_grad)\n",
    "\n",
    "fake_loss = in1_c_grad_gelu.sum() + in2_c_grad_gelu.sum() + w_c_grad_gelu.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dot(fake_loss, params={\"weight\":weight_c, \"in1\":in1_c, \"in2\":in2_c, \"new_out\":new_out, }, show_attrs=True, show_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "upath_cnt = len(in1_idxing)-1\n",
    "mem_dF_dO = torch.zeros_like(new_out)\n",
    "mem_dL_dW =  torch.zeros_like(weight_c)\n",
    "mem_dL_din1 =  torch.zeros_like(in1_c)\n",
    "mem_dL_din2 = torch.zeros((batch_size, i_in2.dim * upath_cnt), device=\"cuda\")\n",
    "mem_debug =  torch.zeros_like(new_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd_bwd.sptp_linear_bwd_bwd_v2_shared(in1_c_grad.grad, in2_c_grad.grad, w_c_grad.grad, new_out, in1_c, in2_c, weight_c, \n",
    "                                           mem_dF_dO, mem_dL_dW, mem_dL_din1, mem_dL_din2, mem_debug,\n",
    "                                           t_in1_idxing, t_in1_ival, t_in1_related_path_idx, \n",
    "                                           \n",
    "                                           t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight,t_per_path_weight_pos, \n",
    "                                           t_per_upath_fiber_array,t_unique_cg_val, \n",
    "                                           upath_cnt, 1, l_max*2+1\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dl_din2_summed = mem_dL_din2.reshape((batch_size, upath_cnt, i_in2.dim)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good\n",
      "All Good\n",
      "All Good\n",
      "All Good\n"
     ]
    }
   ],
   "source": [
    "compare(in1_c.grad, mem_dL_din1)\n",
    "compare(in2_c.grad, mem_dl_din2_summed)\n",
    "compare(weight_c.grad, mem_dL_dW)\n",
    "compare(new_out.grad, mem_dF_dO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_din1_result_list = {}\n",
    "dL_din2_result_list = {}\n",
    "dL_dw_result_list = []\n",
    "dF_dOut_result_list = []\n",
    "dF_uvuv_result_list = []\n",
    "\n",
    "path_id = 0\n",
    "for (l1, l2, l3, parity), (\n",
    "            (mul_1, input_dim1, slice_1),\n",
    "            (mul_2, input_dim2, slice_2),\n",
    "            (output_mul, output_dim, _),\n",
    "        ) in bwd_uvu.paths.items():\n",
    "    w_slice = bwd_uvu.weight_slice[path_id]\n",
    "    \n",
    "    if slice_1.start not in dL_din1_result_list:\n",
    "        dL_din1_result_list[slice_1.start] = []\n",
    "    if slice_2.start not in dL_din2_result_list:\n",
    "        dL_din2_result_list[slice_2.start] = []\n",
    "\n",
    "    dF_in1 = in1_c_grad.grad[:, slice_1].reshape(-1, mul_1, input_dim1)\n",
    "    dF_in2 = in2_c_grad.grad[:, slice_2].reshape(-1, mul_2, input_dim2)\n",
    "    dF_dw = w_c_grad.grad[:, w_slice].reshape(-1, mul_1, mul_2)\n",
    "\n",
    "    in1 = in1_c[:, slice_1].reshape(-1, mul_1, input_dim1)\n",
    "    in2 = in2_c[:, slice_2].reshape(-1, mul_2, input_dim2)\n",
    "    current_w = weight_c[:, w_slice].reshape(-1, mul_1, mul_2)\n",
    "    cg = getattr(bwd_uvu, f\"cg_{l1}_{l2}_{l3}\")    \n",
    "    \n",
    "    dE_dOuter = bwd_uvu.dL_douter_result_list[path_id]\n",
    "    dE_dOut = bwd_uvu.used_dL_dO_list[path_id]\n",
    "\n",
    "    fwd_uvuv = bwd_uvu.uvuv_result_list[path_id]\n",
    "    \n",
    "    # dL_dw path\n",
    "    dF_dOuter_1 = torch.einsum(\"zui,zvj -> zuvij\", dF_in1, in2)\n",
    "    dF_dOuter_2 = torch.einsum(\"zui,zvj -> zuvij\", in1, dF_in2)\n",
    "    dF_duvuv = torch.einsum(\"zuvij, ijk -> zuvk\", dF_dOuter_1 + dF_dOuter_2, cg)\n",
    "    \n",
    "    dw = torch.einsum(\"zuvk, zuk -> zuv\", dF_duvuv, dE_dOut)\n",
    "    dL_dw_result_list.append(dw.reshape(batch_size, -1))\n",
    "\n",
    "    # dL_din1, dL_din2 path (direct)\n",
    "    dL_din1_1 = torch.einsum(\"zvj,zuvij -> zui\", dF_in2, dE_dOuter)\n",
    "    dL_din2_1 = torch.einsum(\"zui,zuvij -> zvj\", dF_in1, dE_dOuter)\n",
    "\n",
    "    # w-> uvuv ->in1,in2 (in-direct)\n",
    "    dL_duvuv = torch.einsum(\"zuv, zuk->zuvk\", dF_dw, dE_dOut)\n",
    "    dL_douter = torch.einsum(\"zuvk, ijk -> zuvij\", dL_duvuv, cg)\n",
    "\n",
    "    dL_din1_2 = torch.einsum(\"zvj,zuvij -> zui\", in2, dL_douter)\n",
    "    dL_din2_2 = torch.einsum(\"zui,zuvij -> zvj\", in1, dL_douter)\n",
    "\n",
    "    # dF/dOut\n",
    "    dF_dOut_1 = torch.einsum(\"zuvk,zuv -> zuk\", dF_duvuv, current_w)\n",
    "    dF_dOut_2 = torch.einsum(\"zuv, zuvk -> zuk\", dF_dw, fwd_uvuv)\n",
    "    dF_dOut_result_list.append((dF_dOut_1 + dF_dOut_2).reshape(4096,-1))\n",
    "\n",
    "    dL_din1_result_list[slice_1.start].append((dL_din1_1 + dL_din1_2).reshape(batch_size,-1))\n",
    "    dL_din2_result_list[slice_2.start].append((dL_din2_1 + dL_din2_2).reshape(batch_size,-1))\n",
    "\n",
    "    dF_uvuv_result_list.append(dF_duvuv.reshape(batch_size,-1))\n",
    "\n",
    "    path_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_bwd_in1_grad = torch.cat([torch.stack(dL_din1_result_list[i], dim=0).sum(dim=0) for i in sorted(dL_din1_result_list.keys())],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_bwd_in2_grad = torch.cat([torch.stack(dL_din2_result_list[i], dim=0).sum(dim=0) for i in sorted(dL_din2_result_list.keys())],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_bwd_w_grad = torch.cat(dL_dw_result_list,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_bwd_dF_dOut = torch.cat([dF_dOut_result_list[i] for i in bwd_uvu.sort2path],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good\n",
      "All Good\n",
      "All Good\n",
      "All Good\n"
     ]
    }
   ],
   "source": [
    "compare(bwd_bwd_in1_grad, in1_c.grad)\n",
    "compare(bwd_bwd_in2_grad, in2_c.grad)\n",
    "compare(bwd_bwd_w_grad, weight_c.grad)\n",
    "compare(bwd_bwd_dF_dOut, new_out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9445, 1.0634, 0.8020,  ..., 0.6121, 0.4798, 0.6113],\n",
       "        [0.8888, 0.8327, 0.6529,  ..., 0.6616, 0.5009, 0.6517],\n",
       "        [0.8042, 1.1100, 1.1256,  ..., 0.6438, 0.8135, 0.6496],\n",
       "        ...,\n",
       "        [0.8703, 0.6446, 0.8475,  ..., 0.7668, 0.8547, 0.7213],\n",
       "        [0.8956, 0.6017, 0.6594,  ..., 0.5233, 0.4545, 0.6376],\n",
       "        [0.8948, 1.0392, 0.9801,  ..., 0.6961, 0.7253, 0.7715]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c_grad.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_id = 0\n",
    "dL_duvuv = bwd_uvu.dL_duvuv_result_list[path_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8439]],\n",
       "\n",
       "         [[0.8260]],\n",
       "\n",
       "         [[0.5262]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.5908]],\n",
       "\n",
       "         [[0.6489]],\n",
       "\n",
       "         [[0.6094]]],\n",
       "\n",
       "\n",
       "        [[[0.8814]],\n",
       "\n",
       "         [[0.9400]],\n",
       "\n",
       "         [[0.9327]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.6381]],\n",
       "\n",
       "         [[0.9279]],\n",
       "\n",
       "         [[0.8302]]],\n",
       "\n",
       "\n",
       "        [[[0.9277]],\n",
       "\n",
       "         [[0.7558]],\n",
       "\n",
       "         [[1.0640]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.7010]],\n",
       "\n",
       "         [[0.9876]],\n",
       "\n",
       "         [[0.7433]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1.0055]],\n",
       "\n",
       "         [[0.9282]],\n",
       "\n",
       "         [[0.7122]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.5219]],\n",
       "\n",
       "         [[0.6847]],\n",
       "\n",
       "         [[0.6416]]],\n",
       "\n",
       "\n",
       "        [[[0.5747]],\n",
       "\n",
       "         [[0.5541]],\n",
       "\n",
       "         [[0.5960]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.5846]],\n",
       "\n",
       "         [[0.6137]],\n",
       "\n",
       "         [[0.6138]]],\n",
       "\n",
       "\n",
       "        [[[0.5968]],\n",
       "\n",
       "         [[0.6644]],\n",
       "\n",
       "         [[0.6918]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.6168]],\n",
       "\n",
       "         [[0.6771]],\n",
       "\n",
       "         [[0.7437]]]], device='cuda:0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwd_uvu.uvuv_result_list[path_id].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4185, 0.5361, 0.0444,  ..., 0.6994, 0.4602, 0.5205],\n",
       "        [1.0293, 1.0845, 1.0851,  ..., 1.0326, 1.1685, 1.5280],\n",
       "        [0.4593, 1.0791, 1.2609,  ..., 0.5202, 0.6484, 0.8033],\n",
       "        ...,\n",
       "        [0.9831, 0.7440, 0.8077,  ..., 0.5734, 1.3865, 1.0628],\n",
       "        [0.7272, 0.6967, 0.8282,  ..., 1.4566, 0.7146, 0.6859],\n",
       "        [0.7638, 1.1111, 0.9568,  ..., 0.6994, 0.7759, 1.0551]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_c.grad[:,w_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7874],\n",
       "         [0.1344],\n",
       "         [0.2190],\n",
       "         ...,\n",
       "         [0.4265],\n",
       "         [0.0642],\n",
       "         [0.3462]],\n",
       "\n",
       "        [[0.1740],\n",
       "         [0.5802],\n",
       "         [0.5541],\n",
       "         ...,\n",
       "         [0.5123],\n",
       "         [0.9454],\n",
       "         [0.4302]],\n",
       "\n",
       "        [[0.1658],\n",
       "         [0.7772],\n",
       "         [0.2972],\n",
       "         ...,\n",
       "         [0.3068],\n",
       "         [0.2712],\n",
       "         [0.5246]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5565],\n",
       "         [0.9619],\n",
       "         [0.3850],\n",
       "         ...,\n",
       "         [0.1500],\n",
       "         [0.9578],\n",
       "         [0.4485]],\n",
       "\n",
       "        [[0.9602],\n",
       "         [0.7773],\n",
       "         [0.3057],\n",
       "         ...,\n",
       "         [0.1741],\n",
       "         [0.2345],\n",
       "         [0.1896]],\n",
       "\n",
       "        [[0.3221],\n",
       "         [0.7318],\n",
       "         [0.0559],\n",
       "         ...,\n",
       "         [0.8725],\n",
       "         [0.6714],\n",
       "         [0.9384]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"zuvk, zuk -> zuv\", dL_duvuv, dL_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good\n"
     ]
    }
   ],
   "source": [
    "compare(in1_c_grad,in1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good\n"
     ]
    }
   ],
   "source": [
    "compare(in2_c_grad,in2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Good\n"
     ]
    }
   ],
   "source": [
    "compare(w_c_grad,weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice(0, 64, None) slice(0, 1, None)\n",
      "torch.Size([4096, 64, 1, 1, 1])\n",
      "slice(0, 1, None)\n",
      "tensor([[0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503],\n",
      "        [0.4503]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "path_cnt = 2\n",
    "for (l1, l2, l3, parity), (\n",
    "            (mul_1, input_dim1, slice_1),\n",
    "            (mul_2, input_dim2, slice_2),\n",
    "            (output_mul, output_dim, _),\n",
    "        ) in bwd_uvu.paths.items():\n",
    "    print(slice_1, slice_2)\n",
    "    ss = bwd_uvu.dL_douter_result_list[path_cnt].reshape(batch_size, mul_1,mul_2,input_dim1, input_dim2)\n",
    "    print(ss.shape)\n",
    "    print(slice_2)\n",
    "    print(torch.einsum(\"...uvij, ...vj -> ...ui\", ss[0,] ,in2_c[0,slice_2].reshape(mul_2, input_dim2)))\n",
    "    \n",
    "    path_cnt +=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,  0.4503,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,\n",
       "         0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,\n",
       "         0.5266,  0.1154,  0.4870,  0.5266,  0.1154,  0.4870,  0.5266,  0.1154,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,  1.1290,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,  0.2600,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,\n",
       "         0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627,\n",
       "        -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908,\n",
       "        -0.2627, -0.0280,  0.2908, -0.2627, -0.0280,  0.2908, -0.2627, -0.0280],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c_grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794, 1.5794,\n",
       "        1.5794, 1.5794, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378, 0.5239, 0.3474, 1.0378,\n",
       "        0.5239, 0.3474, 1.0378, 0.5239, 0.3474], device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m uvuv_gen \u001b[38;5;241m=\u001b[39m (O \u001b[38;5;241m/\u001b[39m W)\u001b[38;5;241m.\u001b[39mreshape(uvuv\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m cg \u001b[38;5;241m=\u001b[39m grad_uvu\u001b[38;5;241m.\u001b[39mused_cg_list[path_idx]\n\u001b[0;32m---> 13\u001b[0m dL_duvuv \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzuk,zuv -> zuvk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdL_dO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m dL_dW \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzuk,zuvk -> zuv\u001b[39m\u001b[38;5;124m\"\u001b[39m, dL_dO, uvuv_gen)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# only place with sparsity\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    404\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "path_idx = 4\n",
    "dL_dO = torch.cat([x.reshape(batch_size,-1) for x in grad_uvu.w_result_list],dim=1)\n",
    "W = grad_uvu.used_w_list[path_idx]\n",
    "\n",
    "\n",
    "# dL_dO = grad_uvu.w_result_list[path_idx].grad\n",
    "W = grad_uvu.used_w_list[path_idx]\n",
    "O = grad_uvu.w_result_list[path_idx]\n",
    "uvuv = grad_uvu.cg_result_list[path_idx]\n",
    "uvuv_gen = (O / W).reshape(uvuv.shape)\n",
    "cg = grad_uvu.used_cg_list[path_idx]\n",
    "\n",
    "dL_duvuv = torch.einsum(\"zuk,zuv -> zuvk\", dL_dO, W)\n",
    "dL_dW = torch.einsum(\"zuk,zuvk -> zuv\", dL_dO, uvuv_gen)\n",
    "# only place with sparsity\n",
    "dL_dOuter = torch.einsum(\"zuvk, ijk -> zuvij\", dL_duvuv, cg)\n",
    "# dL_dA = torch.einsum(\"zuvij,zvj -> zui\", dL_dOuter, in2)\n",
    "# dL_dB = torch.einsum(\"zuvij,zui -> zvj\", dL_dOuter, in1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(out[0], out_exp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_exp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in1 = torch.autograd.grad(y, ori_r, create_graph=True)\n",
    "# f_in1 = dd.forward(ori_r)\n",
    "fake_f_truth = torch.rand_like(f_in1[0], requires_grad=False) - 0.5 \n",
    "fake_f_truth *= (f_in1[0].detach().abs().mean() / fake_f_truth.abs().mean())\n",
    "\n",
    "gated_f_in1 = torch.nn.functional.gelu(f_in1[0])\n",
    "gated_f_in1.retain_grad()\n",
    "f_loss = (gated_f_in1-fake_f_truth).sum()\n",
    "\n",
    "# retain_grad_macro([out, y, *f_in1,f_loss])\n",
    "f_loss.retain_grad()\n",
    "\n",
    "\n",
    "f_loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in1 = torch.rand(batch_size, i_in1.dim, device=\"cuda\", requires_grad=True)\n",
    "# in2 = torch.rand(batch_size, i_in2.dim, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "# ## weight is tp.instruction order not out order\n",
    "# weight = torch.rand(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "# # weight = torch.ones(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "# # grad_uvu = grad_uvu.cuda()\n",
    "\n",
    "# in1_c = in1.clone().detach()\n",
    "# in2_c = in2.clone().detach()\n",
    "# weight_c = weight.clone().detach()\n",
    "\n",
    "# in1_c.requires_grad =True\n",
    "# in2_c.requires_grad =True\n",
    "# weight_c.requires_grad =True\n",
    "\n",
    "# tp = tp.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_c = tp(in1_c,in2_c,weight_c)\n",
    "# out = grad_uvu(in1,in2,weight)\n",
    "out_c.retain_grad()\n",
    "y_c = out_c.sum()\n",
    "y_c.retain_grad()\n",
    "y_c.backward()\n",
    "print(in2_c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(in1_grads, show_attrs=True, show_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upath_cnt = len(in1_idxing)-1\n",
    "mem_dl_din1 = torch.zeros_like(in1)\n",
    "mem_dl_din2 = torch.zeros((batch_size, i_in2.dim * upath_cnt), device=\"cuda\")\n",
    "mem_dl_dw = torch.zeros_like(weight)\n",
    "# mem_dl_do = torch.cat([x.grad.reshape(batch_size,-1) for x in grad_uvu.w_result_list],dim=1)\n",
    "mem_dl_do = torch.ones((batch_size, i_out.dim), device=\"cuda\")\n",
    "mem_debug = torch.ones((batch_size, i_out.dim), device=\"cuda\") * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_out = torch.ops.sptp_linear.sptp_linear_fwd_v2_shared(\n",
    "    in1_c,in2_c,weight_c, \n",
    "    t_in1_idxing, t_in1_ival, t_in1_related_path_idx, \n",
    "    t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, \n",
    "    t_per_upath_fiber_array,t_unique_cg_val, \n",
    "    upath_cnt, 1, l_max, i_out.dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_y = ops_out.sum()\n",
    "ops_y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(in1.grad,in1_c.grad)\n",
    "compare(in2.grad,in2_c.grad)\n",
    "compare(weight.grad,weight_c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_debug = torch.zeros_like(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_fwd_v2_shared(in1,in2,weight, out_debug,\n",
    "                            t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, 1, l_max*2+1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(out[0],out_debug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_fwd_v2(in1,in2,weight, out_debug,\n",
    "                            t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, 1,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(out[0], out_debug[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out2 = grad_uvu(in1,in2,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum = 0\n",
    "# for x in grad_uvu.w_result_list:\n",
    "#     sum += x.reshape(batch_size,-1).shape[1]\n",
    "# print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_bwd_v1(in1,in2,weight, out.grad, mem_dl_din1, mem_dl_din2, mem_dl_dw, mem_debug,\n",
    "                            t_in1_idxing, t_in1_ival , t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, 1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_bwd_v1_shared(in1,in2,weight, out.grad, mem_dl_din1, mem_dl_din2, mem_dl_dw, mem_debug,\n",
    "                            t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, 2, l_max*2+1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare( in1.grad[0], mem_dl_din1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare( weight.grad, mem_dl_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dl_din2_summed = mem_dl_din2.reshape((batch_size, upath_cnt, i_in2.dim)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare( in2.grad, mem_dl_din2_summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END OF FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (i, j) in enumerate(zip(out[0], mem_debug[0])):\n",
    "    print(idx, torch.isclose(i, j).item(), i.item(), j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (i, j) in enumerate(zip(weight.grad[0], mem_dl_dw[0])):\n",
    "    print(idx, torch.isclose(i, j).item(), i.item(), j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_in1_related_path_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_idx = 6\n",
    "t_path_array1[path_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_per_path_fiber_start[path_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fiber_array[14:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_unique_cg_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = tp.instructions[5]\n",
    "print(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cg_mat[f\"{i_in1[inst.i_in1][1].l}_{i_in2[inst.i_in2][1].l}_{i_out[inst.i_out][1].l}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (i, j) in enumerate(zip(out[3], mem_debug[3])):\n",
    "    print(idx, torch.isclose(i, j).item(), i.item(), j.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dummy = torch.zeros(i_out.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(t_path_array1):\n",
    "    dd = i.to(int).tolist()\n",
    "    j_start = t_path_array2[idx][1]\n",
    "    j_val = t_path_array2[idx][2]\n",
    "    print(j_val.item())\n",
    "    # correct_dummy[dd[0]:dd[1]] = t_fiber_array[t_per_path_fiber_start[idx][0]][1]\n",
    "    fiber_y = t_fiber_array[t_per_path_fiber_start[idx][0]][1].item()\n",
    "    correct_dummy[dd[0]:dd[1]] = in2[0][j_start.item()+fiber_y]\n",
    "print(correct_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in enumerate(mem_debug[0]):\n",
    "    print(idx, x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_idx = 3\n",
    "path_start = in1_related_path_idx[in1_idx]\n",
    "path_end = in1_related_path_idx[in1_idx+1]\n",
    "for path_idx in range(path_start, path_end):\n",
    "    for fiber_idx in range(per_path_fiber_start[path_idx][0], per_path_fiber_start[path_idx][1]):\n",
    "        print(fiber_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_array1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mem_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_idx = 4\n",
    "dL_dO = torch.cat([x.reshape(batch_size,-1) for x in grad_uvu.w_result_list],dim=1)\n",
    "W = grad_uvu.used_w_list[path_idx]\n",
    "\n",
    "\n",
    "# dL_dO = grad_uvu.w_result_list[path_idx].grad\n",
    "W = grad_uvu.used_w_list[path_idx]\n",
    "O = grad_uvu.w_result_list[path_idx]\n",
    "uvuv = grad_uvu.cg_result_list[path_idx]\n",
    "uvuv_gen = (O / W).reshape(uvuv.shape)\n",
    "cg = grad_uvu.used_cg_list[path_idx]\n",
    "\n",
    "dL_duvuv = torch.einsum(\"zuk,zuv -> zuvk\", dL_dO, W)\n",
    "dL_dW = torch.einsum(\"zuk,zuvk -> zuv\", dL_dO, uvuv_gen)\n",
    "# only place with sparsity\n",
    "dL_dOuter = torch.einsum(\"zuvk, ijk -> zuvij\", dL_duvuv, cg)\n",
    "# dL_dA = torch.einsum(\"zuvij,zvj -> zui\", dL_dOuter, in2)\n",
    "# dL_dB = torch.einsum(\"zuvij,zui -> zvj\", dL_dOuter, in1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(mem_dl_dw[0]):\n",
    "    print(idx, i.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dl_din1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dl_din2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in enumerate(mem_dl_din1[0]):\n",
    "    print(idx, x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edgepair (num_node, max_neighbour):\n",
    "    edge_src = []\n",
    "    edge_dst = []\n",
    "    for i in range(num_node):\n",
    "        num_neighbour = random.randint(0,max_neighbour)\n",
    "        for j in range(num_neighbour):\n",
    "            edge_dst.append(i)\n",
    "            src_idx = i\n",
    "            while(src_idx == i):\n",
    "                src_idx = random.randint(0,num_node-1)\n",
    "            edge_src.append(src_idx)\n",
    "    return edge_src, edge_dst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_node = 128\n",
    "max_neighbour = 64\n",
    "edge_src, edge_dst = generate_edgepair(total_node, max_neighbour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IR = 11\n",
    "MAX_IN2 = 36\n",
    "MAX_IN1_IR_CNT = 32\n",
    "MAX_NUM_PATH = 512\n",
    "MAX_U_FIBER_CNT = 5265\n",
    "MAX_U_CG_VAL_CNT = 344\n",
    "\n",
    "print(MAX_IN1_IR_CNT*12 + MAX_NUM_PATH*20 + MAX_U_FIBER_CNT*4 + MAX_U_CG_VAL_CNT*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAX_IR*5+MAX_IN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_uvu.inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_w = grad_uvu.used_w_list[0]\n",
    "dl_dO = grad_uvu.w_result_list[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = 1\n",
    "# uvuv can easily retrievd by dividing it with path_weight * weight as v is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_idx = 4\n",
    "dL_dO = grad_uvu.w_result_list[path_idx].grad\n",
    "W = grad_uvu.used_w_list[path_idx]\n",
    "O = grad_uvu.w_result_list[path_idx]\n",
    "uvuv = grad_uvu.cg_result_list[path_idx]\n",
    "uvuv_gen = (O / W).reshape(uvuv.shape)\n",
    "cg = grad_uvu.used_cg_list[path_idx]\n",
    "\n",
    "dL_duvuv = torch.einsum(\"zuk,zuv -> zuvk\", dL_dO, W)\n",
    "dL_dW = torch.einsum(\"zuk,zuvk -> zuv\", dL_dO, uvuv_gen)\n",
    "# only place with sparsity\n",
    "dL_dOuter = torch.einsum(\"zuvk, ijk -> zuvij\", dL_duvuv, cg)\n",
    "# dL_dA = torch.einsum(\"zuvij,zvj -> zui\", dL_dOuter, in2)\n",
    "# dL_dB = torch.einsum(\"zuvij,zui -> zvj\", dL_dOuter, in1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_duvuv = torch.einsum(\"zuk,zuv -> zuvk\", dl_dO, back_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dw = torch.einsum(\"zuk,zuvk -> zuv\", dl_dO, grad_uvu.cg_result_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(correct_out.shape)\n",
    "loss = (correct_out-y)[0].sum()\n",
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fiber = 0\n",
    "for key,val in unique_cg_mat.items():\n",
    "    # print(key, val.count_nonzero().item())\n",
    "    total_fiber += val.count_nonzero().item()\n",
    "print(total_fiber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.weight_numel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tp.instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cg_lookup = list(set([x.item() for x in unique_cg]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_cg_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5265 * 4 / 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_dummy.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39104 * (1+1+1) / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_dummy_kij = cg_dummy.permute(2,0,1)\n",
    "cg_dummy_kij_flat = cg_dummy_kij.reshape(uvuv_i_out.dim, -1)\n",
    "cg_kij_pos_tuple = cg_dummy_kij.nonzero(as_tuple=True)\n",
    "cg_kij_pos_feed = cg_dummy_kij.nonzero().to(dtype=torch.int32).contiguous()\n",
    "cg_kij_val_feed = cg_dummy_kij[cg_kij_pos_tuple]\n",
    "per_out_fiber = cg_dummy_kij_flat.count_nonzero(dim=1)\n",
    "unique_cg_lookup = list(set([x.item() for x in unique_cg]))\n",
    "\n",
    "uni_cg_cnt = len(unique_cg_lookup)\n",
    "out_size = uvuv_i_out.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_occurance = torch.bincount(cg_kij_pos_tuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in2_occurance = torch.bincount(cg_kij_pos_tuple[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_threadblock_data = {}\n",
    "i_out_slice = i_out.slices()\n",
    "for inst in tp.instructions:\n",
    "    if inst.i_in1 not in per_threadblock_data.keys():\n",
    "        per_threadblock_data[inst.i_in1] = {}\n",
    "        per_threadblock_data[inst.i_in1][\"out_slice\"] = []\n",
    "        per_threadblock_data[inst.i_in1][\"\"] = 0\n",
    "    per_threadblock_data[inst.i_in1][\"out_slice\"].append(i_out_slice[inst.i_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_dummy_kij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac2nnz_ij = torch.bincount(cg_dummy_kij_flat.nonzero()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac2nnz_ij.to(float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_cnt_for_each_path = []\n",
    "for inst in tp.instructions:\n",
    "    nnz_cnt_for_each_path.append((inst.i_in1, i_in1[inst.i_in1].dim//32, i_in2[inst.i_in2].dim, cg_dummy_kij_flat[i_out_slice[inst.i_out]].count_nonzero()//32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz_cnt_for_each_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_0_ij_nnz = torch.cat([cg_dummy_kij_flat[s] for s in per_threadblock_data[5][\"out_slice\"]], dim=0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(in1_0_ij_nnz[:,1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in in1_0_ij_nnz:\n",
    "    print(x[:,1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in2_occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_in1_fiber = cg_dummy_kij_flat.count_nonzero(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = torch.rand(batch_size, i_in1.dim)\n",
    "in2 = torch.rand(batch_size, i_in2.dim)\n",
    "out = torch.zeros((batch_size,uvuv_i_out.dim))\n",
    "\n",
    "uni_w3j = torch.tensor(unique_cg_lookup)\n",
    "\n",
    "if(tp.shared_weights):\n",
    "    weight = torch.rand(tp.weight_numel)\n",
    "else:\n",
    "    # weight = torch.rand(batch_size,tp.weight_numel)\n",
    "    weight = torch.ones(batch_size,tp.weight_numel)\n",
    "    # weight[:,928:960] = 2\n",
    "\n",
    "path_weight = torch.zeros(i_out.dim) \n",
    "for inst in tp.instructions:\n",
    "    k = inst.i_out\n",
    "    path_weight[i_out.slices()[k]] = inst.path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blk_u_in1_idx = []\n",
    "blk_u_in2_idx = []\n",
    "\n",
    "per_blk_u_in1_idx_range = []\n",
    "per_blk_u_in2_idx_range = []\n",
    "per_blk_fiber_start_range = []\n",
    "\n",
    "per_fiber_local_idx = []\n",
    "per_fiber_global_idx = []\n",
    "\n",
    "max_u_in1_dim = 0\n",
    "max_u_in2_dim = 0 \n",
    "max_fiber_cnt = 0\n",
    "\n",
    "pattern_len_array = []\n",
    "stride_mul_array = []\n",
    "rem_cum_idx_array = []\n",
    "rem_cumval_array = []\n",
    "fiber_cnt_array = []\n",
    "\n",
    "step = 32\n",
    "i = 0\n",
    "while(i< uvuv_i_out.dim):\n",
    "    local_k_idx = cg_dummy_kij[i:i+step].nonzero()[:,0]\n",
    "    blk_in1_idx = cg_dummy_kij[i:i+step].nonzero()[:,1]\n",
    "    blk_in2_idx = cg_dummy_kij[i:i+step].nonzero()[:,2]\n",
    "\n",
    "    u_in1_list = blk_in1_idx.unique().tolist()\n",
    "    u_in2_list = blk_in2_idx.unique().tolist()\n",
    "\n",
    "    max_u_in1_dim = max(max_u_in1_dim, len(u_in1_list))\n",
    "    max_u_in2_dim = max(max_u_in2_dim, len(u_in2_list))\n",
    "    max_fiber_cnt = max(max_fiber_cnt, len(blk_in1_idx))\n",
    "\n",
    "    blk_u_in1_idx.append(u_in1_list)\n",
    "    blk_u_in2_idx.append(u_in2_list)\n",
    "\n",
    "    per_blk_u_in1_idx_range.append(len(u_in1_list))\n",
    "    per_blk_u_in2_idx_range.append(len(u_in2_list))\n",
    "\n",
    "    per_blk_fiber_start_range.append(len(blk_in1_idx))\n",
    "\n",
    "    local_in1_idx = [u_in1_list.index(x.item()) for x in blk_in1_idx]\n",
    "    local_in2_idx = [u_in2_list.index(x.item()) for x in blk_in2_idx]\n",
    "\n",
    "    for l_a,l_b,l_k,g_a,g_b in zip(local_in1_idx,local_in2_idx, local_k_idx,blk_in1_idx,blk_in2_idx):\n",
    "        real_k = i + l_k\n",
    "        cg_idx = unique_cg_lookup.index(cg_dummy_kij[real_k,g_a,g_b])\n",
    "\n",
    "        per_fiber_local_idx.append(l_a)\n",
    "        per_fiber_local_idx.append(l_b)\n",
    "        per_fiber_local_idx.append(cg_idx)\n",
    "        per_fiber_local_idx.append(l_k)\n",
    "\n",
    "        per_fiber_global_idx.append(g_a)\n",
    "        per_fiber_global_idx.append(g_b)\n",
    "        per_fiber_global_idx.append(cg_idx)\n",
    "        per_fiber_global_idx.append(real_k)\n",
    "\n",
    "\n",
    "    pattern = per_out_fiber[i:i+step].tolist()\n",
    "    pattern_length = 0\n",
    "    if ([pattern[0]]*len(pattern) == pattern):\n",
    "        pattern_length=1\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:3]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_length=3\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:5]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_length=5\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:7]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_length=7\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:9]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_length=9\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:11]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_length=11\n",
    "    else:\n",
    "        print(\"bad\")\n",
    "\n",
    "    unit_pattern = pattern[0:pattern_length]\n",
    "\n",
    "    pattern_len_array.append(pattern_length)\n",
    "    stride_mul_array.append(sum(unit_pattern))\n",
    "    rem_cum_idx_array.append(pattern_length)\n",
    "\n",
    "    rem_cumval_array.append(cumsum_list(unit_pattern, False))\n",
    "    fiber_cnt_array.append(unit_pattern)\n",
    "\n",
    "    i += step\n",
    "\n",
    "rem_cum_idx_array = cumsum_list(rem_cum_idx_array,False)\n",
    "per_blk_u_in1_idx_range  = cumsum_list(per_blk_u_in1_idx_range)\n",
    "per_blk_u_in2_idx_range  = cumsum_list(per_blk_u_in2_idx_range)\n",
    "per_blk_fiber_start_range = cumsum_list(per_blk_fiber_start_range)\n",
    "\n",
    "rem_cumval_array = list(itertools.chain.from_iterable(rem_cumval_array))\n",
    "fiber_cnt_array = list(itertools.chain.from_iterable(fiber_cnt_array))\n",
    "blk_u_in1_idx = list(itertools.chain.from_iterable(blk_u_in1_idx))\n",
    "blk_u_in2_idx = list(itertools.chain.from_iterable(blk_u_in2_idx))\n",
    "\n",
    "per_fiber_local_idx = torch.tensor(per_fiber_local_idx).to(torch.uint8)\n",
    "per_fiber_global_idx = torch.tensor(per_fiber_global_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_u_in1_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_blk_w_idx = []\n",
    "for w_idx, i in enumerate(uvuv_i_out):\n",
    "    for j in range(i.ir.dim):\n",
    "        per_blk_w_idx.append(w_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cuda = out.to(device=\"cuda\")\n",
    "main1_cuda = to_cuda_list(in1.T.contiguous(),in2.T.contiguous())\n",
    "main2_cuda = to_cuda_list(uni_w3j,weight.T.contiguous(),path_weight)\n",
    "meta_cuda = to_cuda_list(\n",
    "                    per_blk_w_idx,\n",
    "                    per_blk_u_in1_idx_range,\n",
    "                    per_blk_u_in2_idx_range,\n",
    "                    per_blk_fiber_start_range,\n",
    "                    \n",
    "                    blk_u_in1_idx,\n",
    "                    blk_u_in2_idx,\n",
    "\n",
    "                    pattern_len_array,\n",
    "                    stride_mul_array,\n",
    "                    rem_cum_idx_array,\n",
    "                    rem_cumval_array,\n",
    "                    fiber_cnt_array, \n",
    "                    \n",
    "                    per_fiber_local_idx,\n",
    "                    input_dtype=torch.uint16)\n",
    "size_input = [max_u_in1_dim,max_u_in2_dim,max_fiber_cnt,uni_cg_cnt]\n",
    "size_input = [int(x) for x in size_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp.sptp_linear_v1(*main1_cuda, out_cuda, *main2_cuda,*meta_cuda,*size_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_out = tp(in1,in2,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4033\n",
    "torch.isclose(correct_out[index], out_cuda.cpu()[index]).all()\n",
    "for a,b in zip(correct_out[index], out_cuda.cpu()[index]):\n",
    "    print(torch.isclose(a,b),a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_u_cnt = []\n",
    "b_u_cnt = []\n",
    "fiber_cnt = []\n",
    "i_cnt = []\n",
    "step = 32\n",
    "i = 0\n",
    "fiber_cnt_pattern = {}\n",
    "while(i< uvuv_i_out.dim):\n",
    "    u_in1 = cg_dummy_kij[i:i+step].nonzero()[:,1].unique()\n",
    "    u_in2 = cg_dummy_kij[i:i+step].nonzero()[:,2].unique()\n",
    "\n",
    "\n",
    "    l_a_u_cnt = (cg_dummy_kij[i:i+step].nonzero()[:,1].unique().numel())\n",
    "    l_b_u_cnt = (cg_dummy_kij[i:i+step].nonzero()[:,2].unique().numel())\n",
    "    l_fiber_cnt = (cg_dummy_kij[i:i+step].nonzero().shape[0])\n",
    "\n",
    "    pattern = per_out_fiber[i:i+step].tolist()\n",
    "\n",
    "    # print([pattern[0]]*len(pattern))\n",
    "    \n",
    "\n",
    "    # print(list(itertools.chain.from_iterable(([pattern[0:3]]*len(pattern))))[0:len(pattern)])\n",
    "    # print(list(itertools.chain.from_iterable(([pattern[0:5]]*len(pattern))))[0:len(pattern)])\n",
    "    \n",
    "    if ([pattern[0]]*len(pattern) == pattern):\n",
    "        pattern_tuple = (1, pattern[0])\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:3]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_tuple = (3, pattern[0:3])\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:5]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_tuple = (5, pattern[0:5])\n",
    "    elif (list(itertools.chain.from_iterable(([pattern[0:7]]*len(pattern))))[0:len(pattern)] == pattern):\n",
    "        pattern_tuple = (7, pattern[0:7])\n",
    "    else:\n",
    "        print(\"bad\")\n",
    "    \n",
    "\n",
    "    if str(pattern_tuple) not in fiber_cnt_pattern:\n",
    "        fiber_cnt_pattern[str(pattern_tuple)] = []\n",
    "    fiber_cnt_pattern[str(pattern_tuple)].append(i//32)\n",
    "    # # plt.scatter(cg_dummy_kij[i:i+step].nonzero()[:,0], cg_dummy_kij[i:i+32].nonzero()[:,1])\n",
    "    # # plt.show()\n",
    "    # # plt.close()\n",
    "    # if(l_a_u_cnt > 60):\n",
    "    #     l_a_u_cnt = (cg_dummy_kij[i:i+step//2].nonzero()[:,1].unique().numel())\n",
    "    #     l_b_u_cnt = (cg_dummy_kij[i:i+step//2].nonzero()[:,2].unique().numel())\n",
    "    #     l_fiber_cnt = (cg_dummy_kij[i:i+step//2].nonzero().shape[0])\n",
    "    #     if(l_a_u_cnt > 60):\n",
    "    #         l_a_u_cnt = (cg_dummy_kij[i:i+step//4].nonzero()[:,1].unique().numel())\n",
    "    #         l_b_u_cnt = (cg_dummy_kij[i:i+step//4].nonzero()[:,2].unique().numel())\n",
    "    #         l_fiber_cnt = (cg_dummy_kij[i:i+step//4].nonzero().shape[0])\n",
    "\n",
    "    #         a_u_cnt.append(l_a_u_cnt)\n",
    "    #         b_u_cnt.append(l_b_u_cnt)\n",
    "    #         fiber_cnt.append(l_fiber_cnt)\n",
    "    #         i_cnt.append(step//4)\n",
    "    #         i += step//4\n",
    "    #     else:\n",
    "    #         a_u_cnt.append(l_a_u_cnt)\n",
    "    #         b_u_cnt.append(l_b_u_cnt)\n",
    "    #         fiber_cnt.append(l_fiber_cnt)\n",
    "    #         i_cnt.append(step//2)\n",
    "    #         i += step//2\n",
    "    # else:\n",
    "    a_u_cnt.append(l_a_u_cnt)\n",
    "    b_u_cnt.append(l_b_u_cnt)\n",
    "    fiber_cnt.append(l_fiber_cnt)\n",
    "    i_cnt.append(step)\n",
    "    i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smem_size = []\n",
    "for a, b, f, i in zip(a_u_cnt, b_u_cnt, fiber_cnt, i_cnt):\n",
    "    smem_size.append((32 * (a+b+i+i) +f) * 4 )\n",
    "    print(a,b,f,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(smem_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cueq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
