{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import e3nn\n",
    "from torch.utils.cpp_extension import load\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import cuequivariance as cue\n",
    "import cuequivariance_torch as cuet\n",
    "import random\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home2/lsy/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home2/lsy/.cache/torch_extensions/py311_cu124/sptp_linear_bwd/build.ninja...\n",
      "Building extension module sptp_linear_bwd...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] c++ -MMD -MF sptp_linear_bwd.o.d -DTORCH_EXTENSION_NAME=sptp_linear_bwd -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/include -isystem /home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/include/TH -isystem /home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /home2/lsy/miniconda3/envs/cueq/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/sptp_linear_bwd.cpp -o sptp_linear_bwd.o \n",
      "[2/2] c++ sptp_linear_bwd.o bwd_sptp_linear.cuda.o bwd_sptp_linear_shared.cuda.o fwd_sptp_linear_v2.cuda.o fwd_sptp_linear_v2_shared.cuda.o bwd_bwd_sptp_linear_v2_shared.cuda.o -shared -L/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.4/lib64 -lcudart -o sptp_linear_bwd.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module sptp_linear_bwd...\n"
     ]
    }
   ],
   "source": [
    "os.environ['TORCH_CUDA_ARCH_LIST'] = \"8.0\"\n",
    "sptp_bwd = load(name='sptp_linear_bwd', sources=['/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/sptp_linear_bwd.cpp', \n",
    "                                  '/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/bwd_sptp_linear.cu',\n",
    "                                  '/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/bwd_sptp_linear_shared.cu',\n",
    "                                  '/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/fwd_sptp_linear_v2.cu',\n",
    "                                  '/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/fwd_sptp_linear_v2_shared.cu',\n",
    "                                  '/home2/lsy/mdsim/fused_e3nn/fused_e3nn_kernel/bwd_bwd_sptp_linear_v2_shared.cu',\n",
    "                                  ], \n",
    "                                  extra_cuda_cflags=[\"-lineinfo\"], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\n",
    "    \"sptp_linear::sptp_linear_fwd_v2_shared\",\n",
    "    mutates_args=(),\n",
    "    device_types=\"cuda\",\n",
    ")\n",
    "def _(\n",
    "    in1: torch.Tensor, \n",
    "    in2: torch.Tensor,\n",
    "    weight: torch.Tensor,\n",
    "\n",
    "    t_in1_idxing: torch.Tensor,\n",
    "    t_in1_ival: torch.Tensor,\n",
    "    t_in1_related_path_idx: torch.Tensor,\n",
    "\n",
    "    t_path_array1: torch.Tensor,\n",
    "    t_path_array2: torch.Tensor,\n",
    "    t_per_upath_fiber_start: torch.Tensor,\n",
    "    t_path_weight: torch.Tensor,\n",
    "    t_per_path_weight_pos: torch.Tensor,\n",
    "\n",
    "    t_per_upath_fiber_array: torch.Tensor,\n",
    "    t_unique_cg_val: torch.Tensor,\n",
    "\n",
    "    upath_cnt:int,\n",
    "    per_block_batch:int,\n",
    "    max_ir_dim:int,\n",
    "    out_size:int ) -> torch.Tensor:\n",
    "    batch_size = in1.shape[0]\n",
    "    out = torch.empty((batch_size, out_size), device=in1.device, dtype=in1.dtype)\n",
    "\n",
    "    sptp_bwd.sptp_linear_fwd_v2_shared(in1,in2,weight, out,\n",
    "                                t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, per_block_batch, max_ir_dim*2+1\n",
    "                                )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_e3nn_setup_fwd_context(ctx, inputs, output):\n",
    "    (\n",
    "     in1, \n",
    "     in2,\n",
    "     weight,\n",
    "\n",
    "     t_in1_idxing,\n",
    "     t_in1_ival,\n",
    "     t_in1_related_path_idx,\n",
    "\n",
    "     t_path_array1,\n",
    "     t_path_array2,\n",
    "     t_per_upath_fiber_start,\n",
    "     t_path_weight,\n",
    "     t_per_path_weight_pos,\n",
    "\n",
    "     t_per_upath_fiber_array,\n",
    "     t_unique_cg_val,\n",
    "\n",
    "     upath_cnt,\n",
    "     per_block_batch,\n",
    "     max_ir_dim,\n",
    "     out_size\n",
    "    ) = inputs\n",
    "    ctx.save_for_backward(\n",
    "        in1,in2,weight,\n",
    "        t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val\n",
    "    )\n",
    "    ctx.upath_cnt = upath_cnt\n",
    "    ctx.per_block_batch = per_block_batch\n",
    "    ctx.max_ir_dim = max_ir_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_e3nn_bwd(ctx, grad_output):\n",
    "    (\n",
    "     in1, \n",
    "     in2,\n",
    "     weight,\n",
    "\n",
    "     t_in1_idxing,\n",
    "     t_in1_ival,\n",
    "     t_in1_related_path_idx,\n",
    "\n",
    "     t_path_array1,\n",
    "     t_path_array2,\n",
    "     t_per_upath_fiber_start,\n",
    "     t_path_weight,\n",
    "     t_per_path_weight_pos,\n",
    "\n",
    "     t_per_upath_fiber_array,\n",
    "     t_unique_cg_val\n",
    "    ) = ctx.saved_tensors\n",
    "\n",
    "\n",
    "    grad_list = torch.ops.sptp_linear.sptp_linear_bwd_v2_shared(\n",
    "        in1,\n",
    "        in2,\n",
    "        weight, \n",
    "        grad_output,\n",
    "        t_in1_idxing, \n",
    "        t_in1_ival, \n",
    "        t_in1_related_path_idx, \n",
    "        t_path_array1,\n",
    "        t_path_array2,\n",
    "        t_per_upath_fiber_start,\n",
    "        t_path_weight,\n",
    "        t_per_path_weight_pos,\n",
    "        t_per_upath_fiber_array,\n",
    "        t_unique_cg_val,\n",
    "        ctx.upath_cnt,\n",
    "        ctx.per_block_batch,\n",
    "        ctx.max_ir_dim\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        grad_list[0], # in1_grad\n",
    "        grad_list[1], # in2_grad\n",
    "        grad_list[2], # weight_grad\n",
    "        \n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        \n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        \n",
    "        None,\n",
    "        None,\n",
    "\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\n",
    "    \"sptp_linear::sptp_linear_bwd_v2_shared\",\n",
    "    mutates_args=(),\n",
    "    device_types=\"cuda\",\n",
    ")\n",
    "def _(\n",
    "    in1: torch.Tensor,\n",
    "    in2: torch.Tensor,\n",
    "    weight: torch.Tensor, \n",
    "    grad_output: torch.Tensor,\n",
    "    t_in1_idxing: torch.Tensor, \n",
    "    t_in1_ival: torch.Tensor, \n",
    "    t_in1_related_path_idx: torch.Tensor, \n",
    "    t_path_array1: torch.Tensor,\n",
    "    t_path_array2: torch.Tensor,\n",
    "    t_per_upath_fiber_start: torch.Tensor,\n",
    "    t_path_weight: torch.Tensor,\n",
    "    t_per_path_weight_pos: torch.Tensor,\n",
    "    t_per_upath_fiber_array: torch.Tensor,\n",
    "    t_unique_cg_val: torch.Tensor,\n",
    "    upath_cnt:int,\n",
    "    per_block_batch:int,\n",
    "    max_ir_dim:int\n",
    "    ) -> List[torch.Tensor]:\n",
    "    \n",
    "    batch_size = in1.shape[0]\n",
    "    in2_size = in2.shape[1]\n",
    "\n",
    "    mem_debug = torch.empty((1,1),device=in1.device)\n",
    "    mem_dl_din1 = torch.empty_like(in1)\n",
    "    mem_dl_din2 = torch.empty((batch_size, in2_size * upath_cnt) , device=in1.device, dtype=in1.dtype)\n",
    "    mem_dl_dw = torch.empty_like(weight)\n",
    "\n",
    "\n",
    "    sptp_bwd.sptp_linear_bwd_v1_shared(in1,in2,weight, grad_output.contiguous(), mem_dl_din1, mem_dl_din2, mem_dl_dw, mem_debug,\n",
    "        t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, per_block_batch, max_ir_dim*2+1\n",
    "    )\n",
    "    mem_dl_din2_summed = mem_dl_din2.reshape((batch_size, upath_cnt, in2_size)).sum(dim=1)\n",
    "    \n",
    "    return [mem_dl_din1,mem_dl_din2_summed, mem_dl_dw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\n",
    "    \"sptp_linear::sptp_linear_bwd_bwd_v2_shared\",\n",
    "    mutates_args=(),\n",
    "    device_types=\"cuda\",\n",
    ")\n",
    "def _(\n",
    "    dF_in1: torch.Tensor,\n",
    "    dF_in2: torch.Tensor,\n",
    "    dF_dw: torch.Tensor,\n",
    "    dE_dout: torch.Tensor,\n",
    "    \n",
    "    in1: torch.Tensor, \n",
    "    in2: torch.Tensor,\n",
    "    weight: torch.Tensor,\n",
    "\n",
    "    t_in1_idxing: torch.Tensor,\n",
    "    t_in1_ival: torch.Tensor,\n",
    "    t_in1_related_path_idx: torch.Tensor,\n",
    "\n",
    "    t_path_array1: torch.Tensor,\n",
    "    t_path_array2: torch.Tensor,\n",
    "    t_per_upath_fiber_start: torch.Tensor,\n",
    "    t_path_weight: torch.Tensor,\n",
    "    t_per_path_weight_pos: torch.Tensor,\n",
    "\n",
    "    t_per_upath_fiber_array: torch.Tensor,\n",
    "    t_unique_cg_val: torch.Tensor,\n",
    "\n",
    "    upath_cnt:int,\n",
    "    per_block_batch:int,\n",
    "    max_ir_dim:int) -> List[torch.Tensor]:\n",
    "\n",
    "    batch_size = in2.shape[0]\n",
    "    in2_size = in2.shape[1]\n",
    "\n",
    "    dF_dout = torch.empty_like(dE_dout)\n",
    "    dL_din1 = torch.empty_like(in1)\n",
    "    dL_din2_duplicate = torch.empty((batch_size, in2_size * upath_cnt) , device=in2.device, dtype=in2.dtype)\n",
    "    dL_dw = torch.empty_like(weight)\n",
    "    mem_debug = torch.empty((1,1),device=in1.device)\n",
    "\n",
    "    sptp_bwd.sptp_linear_bwd_bwd_v2_shared(\n",
    "                                dF_in1, dF_in2, dF_dw, dE_dout,\n",
    "                                in1, in2, weight,\n",
    "                                dF_dout, dL_dw, dL_din1, dL_din2_duplicate, mem_debug,\n",
    "                                t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val, upath_cnt, per_block_batch, max_ir_dim*2+1\n",
    "                                )\n",
    "    \n",
    "    dL_din2 = dL_din2_duplicate.reshape((batch_size, upath_cnt, in2_size)).sum(dim=1)\n",
    "    \n",
    "    return [dL_din1, dL_din2, dL_dw, dF_dout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_e3nn_setup_bwd_context(ctx, inputs, output):\n",
    "    (\n",
    "     in1, \n",
    "     in2,\n",
    "     weight,\n",
    "     dE_dout,\n",
    "\n",
    "     t_in1_idxing,\n",
    "     t_in1_ival,\n",
    "     t_in1_related_path_idx,\n",
    "\n",
    "     t_path_array1,\n",
    "     t_path_array2,\n",
    "     t_per_upath_fiber_start,\n",
    "     t_path_weight,\n",
    "     t_per_path_weight_pos,\n",
    "\n",
    "     t_per_upath_fiber_array,\n",
    "     t_unique_cg_val,\n",
    "\n",
    "     upath_cnt,\n",
    "     per_block_batch,\n",
    "     max_ir_dim\n",
    "    ) = inputs   \n",
    "\n",
    "    ctx.save_for_backward(\n",
    "        dE_dout,\n",
    "        in1, in2, weight,\n",
    "        t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val\n",
    "    )\n",
    "    ctx.upath_cnt = upath_cnt\n",
    "    ctx.per_block_batch = per_block_batch\n",
    "    ctx.max_ir_dim = max_ir_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_e3nn_bwd_bwd(ctx, grad_output):\n",
    "    (dE_dout,\n",
    "        in1, in2, weight,\n",
    "        t_in1_idxing, t_in1_ival, t_in1_related_path_idx, t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight, t_per_path_weight_pos, t_per_upath_fiber_array,t_unique_cg_val\n",
    "    ) = ctx.saved_tensors\n",
    "\n",
    "    dF_in1 =  grad_output[0]\n",
    "    dF_in2 =  grad_output[1]\n",
    "    dF_w =  grad_output[2]\n",
    "\n",
    "    grad_list = torch.ops.sptp_linear.sptp_linear_bwd_bwd_v2_shared(\n",
    "        dF_in1,\n",
    "        dF_in2,\n",
    "        dF_w,\n",
    "        dE_dout.detach(),\n",
    "\n",
    "        in1,\n",
    "        in2,\n",
    "        weight, \n",
    "\n",
    "        t_in1_idxing, \n",
    "        t_in1_ival, \n",
    "        t_in1_related_path_idx, \n",
    "\n",
    "        t_path_array1,\n",
    "        t_path_array2,\n",
    "        t_per_upath_fiber_start,\n",
    "        t_path_weight,\n",
    "        t_per_path_weight_pos,\n",
    "\n",
    "        t_per_upath_fiber_array,\n",
    "        t_unique_cg_val,\n",
    "        \n",
    "        ctx.upath_cnt,\n",
    "        ctx.per_block_batch,\n",
    "        ctx.max_ir_dim\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        grad_list[0],\n",
    "        grad_list[1],\n",
    "        grad_list[2], # weight_grad\n",
    "        grad_list[3], # mem_dL_dO_grad\n",
    "        \n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        \n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        \n",
    "        None,\n",
    "        None,\n",
    "\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.library.register_autograd(\n",
    "    \"sptp_linear::sptp_linear_fwd_v2_shared\",\n",
    "    fused_e3nn_bwd,\n",
    "    setup_context=fused_e3nn_setup_fwd_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.library.register_autograd(\n",
    "    \"sptp_linear::sptp_linear_bwd_v2_shared\",\n",
    "    fused_e3nn_bwd_bwd,\n",
    "    setup_context=fused_e3nn_setup_bwd_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_Irreps(mul, i_in):\n",
    "    dd = []\n",
    "    for ori_mul, ir in i_in:\n",
    "        dd.append((ori_mul*mul, (ir.l, ir.p)))\n",
    "    return e3nn.o3.Irreps(dd)\n",
    "def compare(a, b):\n",
    "    isclose = torch.isclose(a, b)\n",
    "    diff_pos = torch.argwhere(isclose == False)\n",
    "    anything_bad = False\n",
    "    for pos in diff_pos:\n",
    "        pos_t = [x for x in pos]\n",
    "        if(abs(a[pos_t] - b[pos_t]) > 1e-6):\n",
    "            anything_bad = True\n",
    "            print(pos)\n",
    "            print(a[pos_t] - b[pos_t] )\n",
    "    if(not anything_bad):\n",
    "        print(\"All Good\")\n",
    "            \n",
    "IR_IN1_IDX = 0\n",
    "IR_IN2_IDX = 1\n",
    "IR_OUT_IDX = 2\n",
    "INST_IDX = 3\n",
    "\n",
    "def load_nequip_config(h, l_max, layer_idx):\n",
    "    filename = f\"/home2/lsy/mdsim/nequip/benchmark_config/4_{h}_{l_max}_p_sc.txt\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        f_in = f.read().split(\"\\n\")\n",
    "\n",
    "    per_layer_dict = dict()\n",
    "    for l_idx, d in enumerate(f_in):\n",
    "        if(d == \"\") : continue\n",
    "        dd = json.loads(d)\n",
    "        per_layer_dict[l_idx] = dd\n",
    "    tp_list = per_layer_dict[layer_idx][\"tp\"]\n",
    "    i_in1 = e3nn.o3.Irreps(tp_list[IR_IN1_IDX])\n",
    "    i_in2 = e3nn.o3.Irreps(tp_list[IR_IN2_IDX])\n",
    "    i_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "    inst_tuple = [tuple(x) for x in tp_list[INST_IDX]]\n",
    "\n",
    "    return i_in1, i_in2, i_out, inst_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nequip_config_e3nn_cueq(h, l_max, layer_idx):\n",
    "    filename = f\"/home2/lsy/mdsim/nequip/benchmark_config/4_{h}_{l_max}_p_sc.txt\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        f_in = f.read().split(\"\\n\")\n",
    "\n",
    "    per_layer_dict = dict()\n",
    "    for l_idx, d in enumerate(f_in):\n",
    "        if(d == \"\") : continue\n",
    "        dd = json.loads(d)\n",
    "        per_layer_dict[l_idx] = dd\n",
    "    tp_list = per_layer_dict[layer_idx][\"tp\"]\n",
    "\n",
    "    ei_in1 = e3nn.o3.Irreps(tp_list[IR_IN1_IDX])\n",
    "    ei_in2 = e3nn.o3.Irreps(tp_list[IR_IN2_IDX])\n",
    "    ei_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "    inst_tuple = [tuple(x) for x in tp_list[INST_IDX]]\n",
    "\n",
    "\n",
    "    # changing mul for each ir.l\n",
    "    new_in1_list = []\n",
    "    new_out_list = []\n",
    "    changed_idx = [[],[]]\n",
    "    # mul_list = {}\n",
    "    mul_list = {0:128, 1:64}\n",
    "\n",
    "    for idx, (mul,ir) in enumerate(ei_in1):\n",
    "        if (ir.l in mul_list):\n",
    "            new_in1_list.append((mul_list[ir.l], ir))\n",
    "            for inst in inst_tuple:\n",
    "                if(idx == inst[0]):\n",
    "                    changed_idx[0].append(inst[2])\n",
    "                    changed_idx[1].append(mul_list[ir.l])\n",
    "        else:\n",
    "            new_in1_list.append((mul, ir))\n",
    "\n",
    "    for idx, (mul,ir) in enumerate(ei_out):\n",
    "        if (idx in changed_idx[0]):\n",
    "            new_out_list.append((changed_idx[1][changed_idx[0].index(idx)], ir))\n",
    "        else:\n",
    "            new_out_list.append((mul, ir))\n",
    "\n",
    "    ei_in1 = e3nn.o3.Irreps(new_in1_list)\n",
    "    ei_out = e3nn.o3.Irreps(new_out_list)\n",
    "\n",
    "    ci_in1 = cue.Irreps(\"O3\", str(ei_in1))\n",
    "    ci_in2 = cue.Irreps(\"O3\", tp_list[IR_IN2_IDX])\n",
    "    ci_out = cue.Irreps(\"O3\", str(ei_out))\n",
    "\n",
    "\n",
    "    return [ei_in1,ei_in2,ei_out,inst_tuple] , [ci_in1,ci_in2,ci_out,inst_tuple]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda_list(*arg, input_dtype = torch.float32):\n",
    "    return_list = []\n",
    "    for item in arg:\n",
    "        if(type(item) == torch.Tensor):\n",
    "            return_list.append(item.to(device=\"cuda\"))\n",
    "        else:\n",
    "            return_list.append(torch.tensor(item,device=\"cuda\", dtype=input_dtype))\n",
    "    return return_list\n",
    "\n",
    "def to_cuda_dict(strname_list, *arg):\n",
    "    return_dict = {}\n",
    "    for item,name in zip(arg,strname_list):\n",
    "        if(type(item) == torch.Tensor):\n",
    "            return_dict[name] = item.to(\"cuda\")\n",
    "        else:\n",
    "            return_dict[name] = torch.tensor(item,device=\"cuda\")\n",
    "    return return_dict\n",
    "\n",
    "def cumsum_list(s, np1 = True):\n",
    "    new_s = []\n",
    "    current = 0\n",
    "    for e in s:\n",
    "        new_s.append(current)\n",
    "        current += e\n",
    "    if(np1):\n",
    "        new_s.append(current)\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "h = 32\n",
    "l_max = 1\n",
    "layer_idx = 2\n",
    "batch_size = 4096\n",
    "# i_in1, i_in2, i_out, inst_tuple = load_nequip_config(h,l_max,layer_idx)\n",
    "# i_in2 = mul_Irreps(3, i_in2)\n",
    "e3nn_config, cueq_config = load_nequip_config_e3nn_cueq(h,l_max,layer_idx)\n",
    "i_in1, i_in2, i_out, inst_tuple = e3nn_config\n",
    "\n",
    "# not really needed for v=1 \n",
    "uvuv_tp = e3nn.o3.FullTensorProduct(i_in1,i_in2, filter_ir_out=i_out)\n",
    "uvuv_i_out = uvuv_tp.irreps_out\n",
    "\n",
    "# split_size = []\n",
    "# reshape_size = []\n",
    "# for inst in uvuv_tp.instructions:\n",
    "#     split_size.append(uvuv_i_out[inst.i_out].dim)\n",
    "#     reshape_size.append([inst.path_shape[0],inst.path_shape[1],uvuv_i_out[inst.i_out][1].dim])\n",
    "# weight_mul = e3nn.o3.experimental.FullTensorProduct_uvu_weight_only(i_in1, i_in2, split_size, reshape_size, filter_ir_out=i_out, irrep_normalization=None, regroup_output=False)\n",
    "# uvw\n",
    "# i_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "# tp = e3nn.o3.FullyConnectedTensorProduct(i_in1,i_in2,i_out,shared_weights=False, internal_weights=False)\n",
    "\n",
    "# # uvu\n",
    "tp = e3nn.o3.TensorProduct(i_in1,i_in2,i_out,inst_tuple,shared_weights=False, internal_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([x[1].l for x in i_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bwd_uvu = e3nn.o3.experimental.FullTensorProduct_bwd_uvu(i_in1,i_in2, filter_ir_out=i_out, irrep_normalization=\"component\", regroup_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_uvu = e3nn.o3.experimental.FullTensorProduct_grad_uvu(i_in1,i_in2, filter_ir_out=i_out, irrep_normalization=None, regroup_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute density tensor(0.2800)\n"
     ]
    }
   ],
   "source": [
    "# full tp -> linear\n",
    "# i_out = full_tp.irreps_out\n",
    "unique_cg = []\n",
    "unique_cg_mat = {}\n",
    "nnz_cg_cnt = 0\n",
    "all_cg_cnt = 0\n",
    "cg_dummy = torch.zeros(i_in1.dim, i_in2.dim, uvuv_i_out.dim)\n",
    "cg_dummy_coverage = torch.zeros(i_in1.dim, i_in2.dim, uvuv_i_out.dim)\n",
    "for inst in uvuv_tp.instructions:\n",
    "    i = inst.i_in1\n",
    "    j = inst.i_in2\n",
    "    k = inst.i_out\n",
    "\n",
    "    mul_in1, ir_in1 = i_in1[i]\n",
    "    mul_in2, ir_in2 = i_in2[j]\n",
    "    mul_out, ir_out = uvuv_i_out[k]\n",
    "\n",
    "    cg = e3nn.o3.wigner_3j(ir_in1.l, ir_in2.l, ir_out.l)\n",
    "    unique_cg += list(cg.unique())\n",
    "    all_cg_cnt+= cg.numel()\n",
    "    nnz_cg_cnt += cg.count_nonzero()\n",
    "\n",
    "    partial_mat_cg = torch.zeros(i_in1[i].dim, i_in2[j].dim, uvuv_i_out[k].dim)\n",
    "    # print(cg)\n",
    "    unique_cg_mat[f\"{ir_in1.l}_{ir_in2.l}_{ir_out.l}\"] = cg\n",
    "    \n",
    "    ## uvuv\n",
    "    for u,v in itertools.product(range(mul_in1), range(mul_in2)):\n",
    "        partial_mat_cg [u*ir_in1.dim:(u+1)*ir_in1.dim,\n",
    "        v*ir_in2.dim:(v+1)*ir_in2.dim,\n",
    "        (u*mul_in2+v)*ir_out.dim:(u*mul_in2+v+1)*ir_out.dim] = cg \n",
    "\n",
    "    cg_dummy[i_in1.slices()[i], i_in2.slices()[j], uvuv_i_out.slices()[k]] = partial_mat_cg\n",
    "    cg_dummy_coverage[i_in1.slices()[i], i_in2.slices()[j], uvuv_i_out.slices()[k]] = 1\n",
    "print(\"compute density\", nnz_cg_cnt / all_cg_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_cg_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cg_val = list(set([x.item() for x in unique_cg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_inst_outorder = sorted(tp.instructions, key=lambda x : x.i_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already duplicate as num_path > num unique cg matrix\n",
    "per_path_fiber_start = [0]\n",
    "per_path_fiber_array = []\n",
    "\n",
    "for inst in tp_inst_outorder:\n",
    "    path_cg = unique_cg_mat[f\"{i_in1[inst.i_in1][1].l}_{i_in2[inst.i_in2][1].l}_{i_out[inst.i_out][1].l}\"]\n",
    "    for i,j,k in path_cg.nonzero():\n",
    "        cg_idx = unique_cg_val.index(path_cg[i,j,k])\n",
    "        per_path_fiber_array.append([i.item(),j.item(),k.item(),cg_idx])\n",
    "    per_path_fiber_start.append(len(per_path_fiber_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_in1_ir_pathinfo = {}\n",
    "for inst in tp_inst_outorder:\n",
    "    if(inst.i_in1 not in per_in1_ir_pathinfo):\n",
    "        per_in1_ir_pathinfo[inst.i_in1] = []\n",
    "    per_in1_ir_pathinfo[inst.i_in1].append([inst.i_out, inst.i_in2, inst.path_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_uv_pair = []\n",
    "weight_uv_pair_sorted_chunk = []\n",
    "out_order = []\n",
    "current = 0\n",
    "\n",
    "for inst in tp.instructions:\n",
    "    weight_uv_pair.append((i_in1[inst.i_in1][0], i_in2[inst.i_in2][0] ))\n",
    "    out_order.append(inst.i_out)\n",
    "for u,v in weight_uv_pair:\n",
    "    weight_uv_pair_sorted_chunk.append(slice(current,current+u*v))\n",
    "    current+=u*v\n",
    "out2weight_order = torch.tensor(out_order).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARPSIZE = 32\n",
    "in1_idxing = [0]\n",
    "in1_ival = []\n",
    "in1_related_path_idx = [0]\n",
    "\n",
    "path_array1 = []\n",
    "path_array2 = []\n",
    "path_weight = []\n",
    "per_path_weight_pos = []\n",
    "\n",
    "per_upath_fiber_start = []\n",
    "per_upath_fiber_array = []\n",
    "\n",
    "in1_slices = i_in1.slices()\n",
    "in2_slices = i_in2.slices()\n",
    "out_slices = i_out.slices()\n",
    "\n",
    "for in1_ir_idx, (mul,ir) in enumerate(i_in1):\n",
    "    assert (mul%WARPSIZE ==0)\n",
    "    in1_idx_start = in1_slices[in1_ir_idx].start\n",
    "    i_val = ir.dim\n",
    "    \n",
    "    if mul >= WARPSIZE:\n",
    "        for i in range(mul//WARPSIZE):\n",
    "            in1_idxing.append(in1_idx_start + WARPSIZE*i_val*(i+1))\n",
    "            in1_ival.append(i_val)\n",
    "            in1_related_path_idx.append(in1_related_path_idx[-1] + len(per_in1_ir_pathinfo[in1_ir_idx]))\n",
    "            \n",
    "            dummy_list = []\n",
    "            dummy_list2 = []\n",
    "            # Bug? TODO:\n",
    "            for out_ir_idx, in2_ir_idx, pw in per_in1_ir_pathinfo[in1_ir_idx]:\n",
    "                # should be in order\n",
    "                fiber_start = per_path_fiber_start[out_ir_idx]\n",
    "                fiber_end = per_path_fiber_start[out_ir_idx+1]\n",
    "                \n",
    "                upath_fiber_start = len(per_upath_fiber_array)\n",
    "                upath_fiber_end = upath_fiber_start + fiber_end - fiber_start\n",
    "\n",
    "                per_upath_fiber_start.append([upath_fiber_start, upath_fiber_end])\n",
    "                # print(fiber_array_orignal[1:4])\n",
    "                # print(fiber_start,fiber_end)\n",
    "                # print(fiber_array_orignal[fiber_start:fiber_end])\n",
    "                \n",
    "                per_upath_fiber_array += per_path_fiber_array[fiber_start:fiber_end]\n",
    "\n",
    "                dummy_list.append([out_slices[out_ir_idx].start + WARPSIZE*i_out[out_ir_idx].ir.dim * i,\n",
    "                                   out_slices[out_ir_idx].start + WARPSIZE*i_out[out_ir_idx].ir.dim * (i+1)\n",
    "                                   ])\n",
    "                dummy_list2.append([i_out[out_ir_idx].ir.dim,\n",
    "                                    in2_slices[in2_ir_idx].start,\n",
    "                                    i_in2[in2_ir_idx].ir.dim,\n",
    "                                    in2_slices[in2_ir_idx].stop])\n",
    "                path_weight.append(pw)\n",
    "                \n",
    "                # TODO:??\n",
    "                per_path_weight_pos.append(weight_uv_pair_sorted_chunk[out2weight_order[out_ir_idx]].start + WARPSIZE*i)\n",
    "\n",
    "            path_array1.append(dummy_list)\n",
    "            path_array2.append(dummy_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_in1_idxing = torch.tensor(in1_idxing, dtype=torch.int32, device=\"cuda\")\n",
    "t_in1_ival = torch.tensor(in1_ival, dtype=torch.int32, device=\"cuda\")\n",
    "t_in1_related_path_idx = torch.tensor(in1_related_path_idx, dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "t_path_array1 = torch.tensor(list(itertools.chain.from_iterable(path_array1)), dtype=torch.uint16, device=\"cuda\")\n",
    "t_path_array2 = torch.tensor(list(itertools.chain.from_iterable(path_array2)), dtype=torch.uint8, device=\"cuda\")\n",
    "t_path_weight = torch.tensor(path_weight, dtype=torch.float32, device=\"cuda\")\n",
    "t_per_path_weight_pos = torch.tensor(per_path_weight_pos, dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "t_per_upath_fiber_start = torch.tensor(per_upath_fiber_start, dtype=torch.uint16, device=\"cuda\")\n",
    "t_per_upath_fiber_array = torch.tensor(per_upath_fiber_array, dtype=torch.uint8, device=\"cuda\")\n",
    "\n",
    "t_unique_cg_val = torch.tensor(unique_cg_val, dtype=torch.float32, device=\"cuda\")\n",
    "upath_cnt = len(in1_idxing)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tp.cuda()\n",
    "# bwd_uvu = bwd_uvu.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(grad):\n",
    "    return grad * 0  # Scale the gradient by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "in1 = torch.rand(batch_size, i_in1.dim, device=\"cuda\", requires_grad=True)\n",
    "in2 = torch.rand(batch_size, i_in2.dim, device=\"cuda\", requires_grad=True)\n",
    "weight = torch.ones(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "# weight = torch.rand(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "in1_c = in1.clone().detach()\n",
    "in2_c = in2.clone().detach()\n",
    "weight_c = weight.clone().detach()\n",
    "in1_c.requires_grad =True\n",
    "in2_c.requires_grad =True\n",
    "weight_c.requires_grad =True\n",
    "\n",
    "in1_b = in1.clone().detach()\n",
    "in2_b = in2.clone().detach()\n",
    "weight_b = weight.clone().detach()\n",
    "in1_b.requires_grad =True\n",
    "in2_b.requires_grad =True\n",
    "weight_b.requires_grad =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7704, 0.6867, 0.8563,  ..., 0.5528, 0.2079, 0.5731],\n",
      "        [0.9822, 0.9533, 0.6543,  ..., 0.5984, 0.8178, 0.3355],\n",
      "        [0.8418, 0.5885, 0.6154,  ..., 0.3496, 0.6252, 0.6307],\n",
      "        ...,\n",
      "        [0.9611, 0.6230, 0.9658,  ..., 0.5955, 0.2656, 0.7359],\n",
      "        [0.9286, 0.9267, 0.7335,  ..., 0.7273, 0.5112, 0.1011],\n",
      "        [0.7893, 0.7277, 0.6409,  ..., 0.4744, 0.4633, 0.5597]],\n",
      "       device='cuda:0', grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_exp = tp(in1,in2,weight)\n",
    "out_exp.retain_grad()\n",
    "y = torch.nn.functional.gelu(out_exp).sum()\n",
    "y.retain_grad()\n",
    "# y.backward()\n",
    "f_in1, f_in2, f_weight = torch.autograd.grad(y, [in1,in2,weight], create_graph=True)\n",
    "f_in1.retain_grad()\n",
    "f_in2.retain_grad()\n",
    "f_weight.retain_grad()\n",
    "print(out_exp.grad)\n",
    "dE_dout =  out_exp.grad.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_exp.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4094213., device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f_in1_gelu = torch.nn.functional.gelu(f_in1)\n",
    "f_in2_gelu = torch.nn.functional.gelu(f_in2)\n",
    "f_weight_gelu = torch.nn.functional.gelu(f_weight)\n",
    "\n",
    "fake_loss = f_in1_gelu.sum() + f_in2_gelu.sum() + f_weight_gelu.sum()\n",
    "print(fake_loss)\n",
    "fake_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dout =  out_exp.grad.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_exp.register_hook(zero_grad)\n",
    "# f_weight.register_hook(zero_grad)\n",
    "# f_in1.register_hook(zero_grad)\n",
    "# f_in2.register_hook(zero_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dL_dW_2 =  torch.zeros_like(weight_c)\n",
    "mem_dL_din1_2 =  torch.zeros_like(in1_c)\n",
    "mem_dL_din2_2 = torch.zeros((batch_size, i_in2.dim * upath_cnt), device=\"cuda\")\n",
    "mem_debug =  torch.zeros_like(out_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_bwd_v1_shared(in1, in2, weight, dL_dout, \n",
    "                                           mem_dL_din1_2, mem_dL_din2_2,mem_dL_dW_2, mem_debug,\n",
    "                                           t_in1_idxing, t_in1_ival, t_in1_related_path_idx, \n",
    "                                           \n",
    "                                           t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight,t_per_path_weight_pos, \n",
    "                                           t_per_upath_fiber_array,t_unique_cg_val, \n",
    "                                           upath_cnt, 1, l_max*2+1\n",
    "                                           )\n",
    "mem_dl_din2_2_summed = mem_dL_din2_2.reshape((batch_size, upath_cnt, i_in2.dim)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dF_dO = torch.zeros_like(out_exp)\n",
    "mem_dL_dW =  torch.zeros_like(weight_c)\n",
    "mem_dL_din1 =  torch.zeros_like(in1_c)\n",
    "mem_dL_din2 = torch.zeros((batch_size, i_in2.dim * upath_cnt), device=\"cuda\")\n",
    "mem_debug =  torch.zeros_like(out_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_bwd_bwd_v2_shared(f_in1.grad, f_in2.grad, f_weight.grad, dE_dout, in1, in2, weight, \n",
    "                                           mem_dF_dO, mem_dL_dW, mem_dL_din1, mem_dL_din2, mem_debug,\n",
    "                                           t_in1_idxing, t_in1_ival, t_in1_related_path_idx, \n",
    "                                           \n",
    "                                           t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight,t_per_path_weight_pos, \n",
    "                                           t_per_upath_fiber_array,t_unique_cg_val, \n",
    "                                           upath_cnt, 1, l_max*2+1\n",
    "                                           )\n",
    "mem_dl_din2_summed = mem_dL_din2.reshape((batch_size, upath_cnt, i_in2.dim)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5773, 6.0905, 3.7802,  ..., 3.9337, 3.0472, 3.8047],\n",
       "        [7.8987, 6.1085, 6.3980,  ..., 1.6678, 3.6508, 4.9597],\n",
       "        [5.3784, 5.7791, 3.7292,  ..., 3.0424, 1.6832, 2.6684],\n",
       "        ...,\n",
       "        [5.8595, 7.0677, 7.4968,  ..., 2.5093, 2.4028, 1.2549],\n",
       "        [8.5208, 8.2731, 7.2394,  ..., 3.7490, 4.9139, 3.5372],\n",
       "        [5.6741, 5.3575, 5.9812,  ..., 2.3310, 2.0964, 2.1986]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5773, 6.0905, 3.7802,  ..., 3.9337, 3.0472, 3.8047],\n",
       "        [7.8987, 6.1085, 6.3980,  ..., 1.6678, 3.6508, 4.9597],\n",
       "        [5.3784, 5.7791, 3.7292,  ..., 3.0424, 1.6832, 2.6684],\n",
       "        ...,\n",
       "        [5.8595, 7.0677, 7.4968,  ..., 2.5093, 2.4028, 1.2549],\n",
       "        [8.5208, 8.2731, 7.2394,  ..., 3.7490, 4.9139, 3.5372],\n",
       "        [5.6741, 5.3575, 5.9812,  ..., 2.3310, 2.0964, 2.1986]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_din1 + mem_dL_din1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ours = torch.ops.sptp_linear.sptp_linear_fwd_v2_shared(\n",
    "    in1_c, in2_c, weight_c,\n",
    "    t_in1_idxing, t_in1_ival, t_in1_related_path_idx,                                           \n",
    "    t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight,t_per_path_weight_pos, \n",
    "    t_per_upath_fiber_array,t_unique_cg_val, \n",
    "    upath_cnt, 1, l_max, i_out.dim\n",
    ")\n",
    "out_ours.retain_grad()\n",
    "y_ours = torch.nn.functional.gelu(out_ours).sum()\n",
    "# y_ours.backward()\n",
    "f_in1_c, f_in2_c, f_weight_c = torch.autograd.grad(y_ours, [in1_c,in2_c,weight_c], create_graph=True)\n",
    "# print(out_ours.grad)\n",
    "dE_dout_c = out_ours.grad.detach().clone()\n",
    "# out_ours.grad = None\n",
    "f_in1_c.retain_grad()\n",
    "f_in2_c.retain_grad() \n",
    "f_weight_c.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in1_c_gelu = torch.nn.functional.gelu(f_in1_c)\n",
    "f_in2_c_gelu = torch.nn.functional.gelu(f_in2_c)\n",
    "f_weight_c_gelu = torch.nn.functional.gelu(f_weight_c)\n",
    "fake_loss_c = f_in1_c_gelu.sum() + f_in2_c_gelu.sum() + f_weight_c_gelu.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_loss_c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_dL_din1_c =  torch.zeros_like(in1_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_bwd_bwd_v2_shared(f_in1_c.grad, f_in2_c.grad, f_weight_c.grad, dE_dout_c, in1, in2, weight, \n",
    "                                           mem_dF_dO, mem_dL_dW, mem_dL_din1_c, mem_dL_din2, mem_debug,\n",
    "                                           t_in1_idxing, t_in1_ival, t_in1_related_path_idx, \n",
    "                                           \n",
    "                                           t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight,t_per_path_weight_pos, \n",
    "                                           t_per_upath_fiber_array,t_unique_cg_val, \n",
    "                                           upath_cnt, 1, l_max*2+1\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5773, 6.0905, 3.7802,  ..., 3.9337, 3.0472, 3.8047],\n",
       "        [7.8987, 6.1085, 6.3980,  ..., 1.6678, 3.6508, 4.9597],\n",
       "        [5.3784, 5.7791, 3.7292,  ..., 3.0424, 1.6832, 2.6684],\n",
       "        ...,\n",
       "        [5.8595, 7.0677, 7.4968,  ..., 2.5093, 2.4028, 1.2549],\n",
       "        [8.5208, 8.2731, 7.2394,  ..., 3.7490, 4.9139, 3.5372],\n",
       "        [5.6741, 5.3575, 5.9812,  ..., 2.3310, 2.0964, 2.1986]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5773, 6.0905, 3.7802,  ..., 3.9337, 3.0472, 3.8047],\n",
       "        [7.8987, 6.1085, 6.3980,  ..., 1.6678, 3.6508, 4.9597],\n",
       "        [5.3784, 5.7791, 3.7292,  ..., 3.0424, 1.6832, 2.6684],\n",
       "        ...,\n",
       "        [5.8595, 7.0677, 7.4968,  ..., 2.5093, 2.4028, 1.2549],\n",
       "        [8.5208, 8.2731, 7.2394,  ..., 3.7490, 4.9139, 3.5372],\n",
       "        [5.6741, 5.3575, 5.9812,  ..., 2.3310, 2.0964, 2.1986]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 1408])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dF_dO.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_in1 tensor([[1.0835, 1.0992, 0.9990,  ..., 0.7717, 0.8860, 0.9795],\n",
      "        [1.1251, 1.1014, 1.1067,  ..., 0.8032, 0.8488, 1.0750],\n",
      "        [0.9974, 1.0096, 0.9414,  ..., 0.8907, 0.7220, 0.7496],\n",
      "        ...,\n",
      "        [0.9995, 1.0388, 1.0745,  ..., 0.6675, 0.9395, 0.6983],\n",
      "        [1.1185, 1.1241, 1.1287,  ..., 0.9539, 0.9254, 0.9626],\n",
      "        [0.9864, 0.9759, 0.9965,  ..., 0.7334, 0.7592, 0.7311]],\n",
      "       device='cuda:0')\n",
      "f_in2 tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='cuda:0')\n",
      "f_weight tensor([[0.6828, 0.7529, 0.5081,  ..., 0.5357, 0.4965, 0.4815],\n",
      "        [0.7294, 0.5938, 0.6124,  ..., 0.5583, 0.5552, 0.6262],\n",
      "        [0.6096, 0.6350, 0.5247,  ..., 0.5151, 0.5374, 0.5493],\n",
      "        ...,\n",
      "        [0.5171, 0.5307, 0.5492,  ..., 0.4736, 0.5697, 0.5699],\n",
      "        [1.0690, 1.0005, 0.8217,  ..., 0.5710, 0.5248, 0.5242],\n",
      "        [0.7259, 0.6936, 0.7597,  ..., 0.5002, 0.5202, 0.5011]],\n",
      "       device='cuda:0')\n",
      "dE_dout tensor([[0.7704, 0.6867, 0.8563,  ..., 0.5305, 0.3258, 0.5423],\n",
      "        [0.9822, 0.9533, 0.6543,  ..., 0.5570, 0.6908, 0.4041],\n",
      "        [0.8418, 0.5885, 0.6154,  ..., 0.4125, 0.5727, 0.5759],\n",
      "        ...,\n",
      "        [0.9611, 0.6230, 0.9658,  ..., 0.5553, 0.3619, 0.6390],\n",
      "        [0.9286, 0.9267, 0.7335,  ..., 0.6338, 0.5065, 0.2540],\n",
      "        [0.7893, 0.7277, 0.6409,  ..., 0.4852, 0.4788, 0.5345]],\n",
      "       device='cuda:0')\n",
      "in1 tensor([[0.3990, 0.5167, 0.0249,  ..., 0.7826, 0.3082, 0.7250],\n",
      "        [0.5399, 0.2629, 0.3050,  ..., 0.1379, 0.4489, 0.9885],\n",
      "        [0.5339, 0.6323, 0.1454,  ..., 0.9781, 0.4150, 0.7312],\n",
      "        ...,\n",
      "        [0.3423, 0.5899, 0.8992,  ..., 0.5487, 0.4920, 0.2722],\n",
      "        [0.9084, 0.7625, 0.5018,  ..., 0.3609, 0.9881, 0.2167],\n",
      "        [0.8826, 0.7806, 0.9867,  ..., 0.6901, 0.5691, 0.6447]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "in2 tensor([[0.7874, 0.1344, 0.2190, 0.8193],\n",
      "        [0.6969, 0.7304, 0.2328, 0.9028],\n",
      "        [0.3900, 0.6177, 0.5012, 0.2330],\n",
      "        ...,\n",
      "        [0.1175, 0.1372, 0.9088, 0.8486],\n",
      "        [0.9871, 0.8377, 0.1289, 0.4479],\n",
      "        [0.4212, 0.2920, 0.3944, 0.3671]], device='cuda:0', requires_grad=True)\n",
      "weight tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', requires_grad=True)\n",
      "dL_din1 tensor([[2.3939, 2.6076, 1.7698,  ..., 1.3619, 1.2394, 1.4429],\n",
      "        [2.9043, 2.3094, 2.3960,  ..., 1.1597, 1.3672, 1.8853],\n",
      "        [2.2201, 2.3311, 1.8035,  ..., 1.3438, 1.0559, 1.1976],\n",
      "        ...,\n",
      "        [2.0722, 2.4082, 2.8295,  ..., 1.0296, 1.1911, 0.9216],\n",
      "        [3.9204, 3.6235, 2.9922,  ..., 1.2741, 1.6628, 1.2556],\n",
      "        [2.4327, 2.3346, 2.5331,  ..., 1.0610, 1.0325, 1.0366]],\n",
      "       device='cuda:0')\n",
      "t_in1_idxing\n",
      "tensor([  0,  32,  64,  96, 128, 224, 320, 416, 512], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "t_in1_ival\n",
      "tensor([1, 1, 1, 1, 3, 3, 3, 3], device='cuda:0', dtype=torch.int32)\n",
      "t_in1_related_path_idx\n",
      "tensor([ 0,  2,  4,  6,  8, 11, 14, 17, 20], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "t_path_array1\n",
      "tensor([[  64,   96],\n",
      "        [ 256,  352],\n",
      "        [  96,  128],\n",
      "        [ 352,  448],\n",
      "        [ 128,  160],\n",
      "        [ 448,  544],\n",
      "        [ 160,  192],\n",
      "        [ 544,  640],\n",
      "        [   0,   32],\n",
      "        [ 640,  736],\n",
      "        [1024, 1120],\n",
      "        [  32,   64],\n",
      "        [ 736,  832],\n",
      "        [1120, 1216],\n",
      "        [ 192,  224],\n",
      "        [ 832,  928],\n",
      "        [1216, 1312],\n",
      "        [ 224,  256],\n",
      "        [ 928, 1024],\n",
      "        [1312, 1408]], device='cuda:0', dtype=torch.uint16)\n",
      "t_path_array2\n",
      "tensor([[1, 0, 1, 1],\n",
      "        [3, 1, 3, 4],\n",
      "        [1, 0, 1, 1],\n",
      "        [3, 1, 3, 4],\n",
      "        [1, 0, 1, 1],\n",
      "        [3, 1, 3, 4],\n",
      "        [1, 0, 1, 1],\n",
      "        [3, 1, 3, 4],\n",
      "        [1, 1, 3, 4],\n",
      "        [3, 1, 3, 4],\n",
      "        [3, 0, 1, 1],\n",
      "        [1, 1, 3, 4],\n",
      "        [3, 1, 3, 4],\n",
      "        [3, 0, 1, 1],\n",
      "        [1, 1, 3, 4],\n",
      "        [3, 0, 1, 1],\n",
      "        [3, 1, 3, 4],\n",
      "        [1, 1, 3, 4],\n",
      "        [3, 0, 1, 1],\n",
      "        [3, 1, 3, 4]], device='cuda:0', dtype=torch.uint8)\n",
      "t_per_upath_fiber_start\n",
      "tensor([[ 0,  1],\n",
      "        [ 1,  4],\n",
      "        [ 4,  5],\n",
      "        [ 5,  8],\n",
      "        [ 8,  9],\n",
      "        [ 9, 12],\n",
      "        [12, 13],\n",
      "        [13, 16],\n",
      "        [16, 19],\n",
      "        [19, 25],\n",
      "        [25, 28],\n",
      "        [28, 31],\n",
      "        [31, 37],\n",
      "        [37, 40],\n",
      "        [40, 43],\n",
      "        [43, 46],\n",
      "        [46, 52],\n",
      "        [52, 55],\n",
      "        [55, 58],\n",
      "        [58, 64]], device='cuda:0', dtype=torch.uint16)\n",
      "t_path_weight\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0')\n",
      "t_per_path_weight_pos\n",
      "tensor([  0, 128,  32, 160,  64, 192,  96, 224, 320, 384, 256, 352, 416, 288,\n",
      "        512, 448, 576, 544, 480, 608], device='cuda:0', dtype=torch.int32)\n",
      "t_per_upath_fiber_array\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 3],\n",
      "        [0, 1, 1, 3],\n",
      "        [0, 2, 2, 3],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 3],\n",
      "        [0, 1, 1, 3],\n",
      "        [0, 2, 2, 3],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 3],\n",
      "        [0, 1, 1, 3],\n",
      "        [0, 2, 2, 3],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 3],\n",
      "        [0, 1, 1, 3],\n",
      "        [0, 2, 2, 3],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 1, 0, 3],\n",
      "        [2, 2, 0, 3],\n",
      "        [0, 1, 2, 2],\n",
      "        [0, 2, 1, 4],\n",
      "        [1, 0, 2, 4],\n",
      "        [1, 2, 0, 2],\n",
      "        [2, 0, 1, 2],\n",
      "        [2, 1, 0, 4],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 0, 1, 3],\n",
      "        [2, 0, 2, 3],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 1, 0, 3],\n",
      "        [2, 2, 0, 3],\n",
      "        [0, 1, 2, 2],\n",
      "        [0, 2, 1, 4],\n",
      "        [1, 0, 2, 4],\n",
      "        [1, 2, 0, 2],\n",
      "        [2, 0, 1, 2],\n",
      "        [2, 1, 0, 4],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 0, 1, 3],\n",
      "        [2, 0, 2, 3],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 1, 0, 3],\n",
      "        [2, 2, 0, 3],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 0, 1, 3],\n",
      "        [2, 0, 2, 3],\n",
      "        [0, 1, 2, 2],\n",
      "        [0, 2, 1, 4],\n",
      "        [1, 0, 2, 4],\n",
      "        [1, 2, 0, 2],\n",
      "        [2, 0, 1, 2],\n",
      "        [2, 1, 0, 4],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 1, 0, 3],\n",
      "        [2, 2, 0, 3],\n",
      "        [0, 0, 0, 3],\n",
      "        [1, 0, 1, 3],\n",
      "        [2, 0, 2, 3],\n",
      "        [0, 1, 2, 2],\n",
      "        [0, 2, 1, 4],\n",
      "        [1, 0, 2, 4],\n",
      "        [1, 2, 0, 2],\n",
      "        [2, 0, 1, 2],\n",
      "        [2, 1, 0, 4]], device='cuda:0', dtype=torch.uint8)\n",
      "t_unique_cg_val\n",
      "tensor([ 0.0000,  1.0000,  0.4082,  0.5774, -0.4082], device='cuda:0')\n",
      "upath_cnt\n",
      "8\n",
      "per_block_batch\n",
      "1\n",
      "lmax*2+1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(\"f_in1\",f_in1.grad)\n",
    "print(\"f_in2\",f_in2.grad)\n",
    "print(\"f_weight\",f_weight.grad)\n",
    "print(\"dE_dout\",dE_dout)\n",
    "print(\"in1\", in1)\n",
    "print(\"in2\", in2)\n",
    "print(\"weight\", weight)\n",
    "print(\"dL_din1\", mem_dL_din1)\n",
    "\n",
    "\n",
    "print(\"t_in1_idxing\")\n",
    "print(t_in1_idxing)\n",
    "print(\"t_in1_ival\")\n",
    "print(t_in1_ival)\n",
    "print(\"t_in1_related_path_idx\")\n",
    "print(t_in1_related_path_idx)\n",
    "print(\"t_path_array1\")\n",
    "print(t_path_array1)\n",
    "print(\"t_path_array2\")\n",
    "print(t_path_array2)\n",
    "print(\"t_per_upath_fiber_start\")\n",
    "print(t_per_upath_fiber_start)\n",
    "print(\"t_path_weight\")\n",
    "print(t_path_weight)\n",
    "print(\"t_per_path_weight_pos\")\n",
    "print(t_per_path_weight_pos)\n",
    "print(\"t_per_upath_fiber_array\")\n",
    "print(t_per_upath_fiber_array)\n",
    "print(\"t_unique_cg_val\")\n",
    "print(t_unique_cg_val)\n",
    "print(\"upath_cnt\")\n",
    "print(upath_cnt)\n",
    "print(\"per_block_batch\")\n",
    "print(1)\n",
    "print(\"lmax*2+1\")\n",
    "print(l_max*2+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2085, 1.3191, 0.7220,  ..., 0.5664, 0.4250, 0.9886],\n",
       "        [1.5657, 1.2067, 1.2670,  ..., 0.5222, 0.5092, 1.1629],\n",
       "        [0.7433, 0.8249, 0.4065,  ..., 0.6783, 0.3948, 0.3597],\n",
       "        ...,\n",
       "        [0.7110, 0.9278, 1.1433,  ..., 0.3238, 0.7436, 0.5749],\n",
       "        [1.6389, 1.7620, 1.7631,  ..., 1.0391, 0.8856, 0.7171],\n",
       "        [0.8572, 0.7910, 0.9210,  ..., 0.3631, 0.3887, 0.3929]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_din1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3939, 2.6076, 1.7698,  ..., 1.3619, 1.2394, 1.4429],\n",
       "        [2.9043, 2.3094, 2.3960,  ..., 1.1597, 1.3672, 1.8853],\n",
       "        [2.2201, 2.3311, 1.8035,  ..., 1.3438, 1.0559, 1.1976],\n",
       "        ...,\n",
       "        [2.0722, 2.4082, 2.8295,  ..., 1.0296, 1.1911, 0.9216],\n",
       "        [3.9204, 3.6235, 2.9922,  ..., 1.2741, 1.6628, 1.2556],\n",
       "        [2.4327, 2.3346, 2.5331,  ..., 1.0610, 1.0325, 1.0366]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_din1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3374,  0.9280,  1.2032,  ...,  0.0454, -0.2542,  0.1906],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_exp.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3374,  0.9280,  1.2032,  ...,  0.0454, -0.2542,  0.1906],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ours.grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6023, 3.9266, 2.4918,  ..., 1.9283, 1.6644, 2.4315],\n",
       "        [4.4700, 3.5160, 3.6630,  ..., 1.6819, 1.8764, 3.0482],\n",
       "        [2.9633, 3.1560, 2.2101,  ..., 2.0220, 1.4507, 1.5573],\n",
       "        ...,\n",
       "        [2.7832, 3.3361, 3.9729,  ..., 1.3533, 1.9347, 1.4965],\n",
       "        [5.5593, 5.3854, 4.7553,  ..., 2.3132, 2.5484, 1.9727],\n",
       "        [3.2900, 3.1256, 3.4540,  ..., 1.4241, 1.4212, 1.4295]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1383, 2.4349, 1.1404,  ..., 2.3862, 1.4359, 2.6144],\n",
       "        [2.4970, 2.2493, 2.2964,  ..., 2.1748, 3.0055, 3.1140],\n",
       "        [0.9395, 0.8798, 0.6203,  ..., 1.5471, 0.9014, 1.4099],\n",
       "        ...,\n",
       "        [1.8932, 1.6205, 2.1985,  ..., 0.3500, 0.8847, 1.1050],\n",
       "        [2.5475, 2.3638, 2.3195,  ..., 1.0526, 0.9460, 0.8123],\n",
       "        [1.1587, 1.1379, 1.8229,  ..., 1.5200, 1.1463, 0.8046]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2085, 1.3191, 0.7220,  ..., 0.5664, 0.4250, 0.9886],\n",
       "        [1.5657, 1.2067, 1.2670,  ..., 0.5222, 0.5092, 1.1629],\n",
       "        [0.7433, 0.8249, 0.4065,  ..., 0.6783, 0.3948, 0.3597],\n",
       "        ...,\n",
       "        [0.7110, 0.9278, 1.1433,  ..., 0.3238, 0.7436, 0.5749],\n",
       "        [1.6389, 1.7620, 1.7631,  ..., 1.0391, 0.8856, 0.7171],\n",
       "        [0.8572, 0.7910, 0.9210,  ..., 0.3631, 0.3887, 0.3929]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_din1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3939, 2.6076, 1.7698,  ..., 1.3619, 1.2394, 1.4429],\n",
       "        [2.9043, 2.3094, 2.3960,  ..., 1.1597, 1.3672, 1.8853],\n",
       "        [2.2201, 2.3311, 1.8035,  ..., 1.3438, 1.0559, 1.1976],\n",
       "        ...,\n",
       "        [2.0722, 2.4082, 2.8295,  ..., 1.0296, 1.1911, 0.9216],\n",
       "        [3.9204, 3.6235, 2.9922,  ..., 1.2741, 1.6628, 1.2556],\n",
       "        [2.4327, 2.3346, 2.5331,  ..., 1.0610, 1.0325, 1.0366]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_din1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1383, 2.4349, 1.1404,  ..., 2.3862, 1.4359, 2.6144],\n",
       "        [2.4970, 2.2493, 2.2964,  ..., 2.1748, 3.0055, 3.1140],\n",
       "        [0.9395, 0.8798, 0.6203,  ..., 1.5471, 0.9014, 1.4099],\n",
       "        ...,\n",
       "        [1.8932, 1.6205, 2.1985,  ..., 0.3500, 0.8847, 1.1050],\n",
       "        [2.5475, 2.3638, 2.3195,  ..., 1.0526, 0.9460, 0.8123],\n",
       "        [1.1587, 1.1379, 1.8229,  ..., 1.5200, 1.1463, 0.8046]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dot(f_in1, params={\"in1\":in1, \"in2\":in2, \"weight\":weight, \"f_in1\":f_in1}).render(\"attached\", format=\"png\")\n",
    "# make_dot(f_in1, params={\"in1\":in1, \"in2\":in2, \"weight\":weight, \"f_in1\":f_in1}, show_attrs=True, show_saved=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7816, 2.0296, 1.0832,  ..., 1.4670, 1.0924, 0.9306],\n",
       "        [0.9562, 1.3873, 0.8707,  ..., 0.7697, 1.1433, 1.0237],\n",
       "        [0.1781, 0.6431, 0.9443,  ..., 0.8144, 0.6786, 0.9715],\n",
       "        ...,\n",
       "        [1.3204, 0.6390, 1.1996,  ..., 1.6154, 0.5895, 0.7243],\n",
       "        [0.4803, 1.2466, 1.2268,  ..., 0.1781, 0.3921, 0.4132],\n",
       "        [1.2405, 0.5305, 0.8399,  ..., 1.2313, 0.6930, 0.6711]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dF_dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1880, 1.4026, 0.5561,  ..., 1.7543, 1.0620, 1.9509],\n",
       "        [0.5432, 0.8636, 0.4849,  ..., 1.0886, 0.5023, 1.6023],\n",
       "        [0.0880, 0.3345, 0.5114,  ..., 0.6527, 0.8226, 1.3222],\n",
       "        ...,\n",
       "        [0.8195, 0.3325, 0.7256,  ..., 0.4892, 0.7790, 1.6820],\n",
       "        [0.2458, 0.7442, 0.7297,  ..., 0.6555, 1.4635, 0.4917],\n",
       "        [0.7267, 0.2733, 0.4580,  ..., 0.4321, 0.6894, 1.4175]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1880, 1.4026, 0.5561,  ..., 1.7543, 1.0620, 1.9509],\n",
       "        [0.5432, 0.8636, 0.4849,  ..., 1.0886, 0.5023, 1.6023],\n",
       "        [0.0880, 0.3345, 0.5114,  ..., 0.6527, 0.8226, 1.3222],\n",
       "        ...,\n",
       "        [0.8195, 0.3325, 0.7256,  ..., 0.4892, 0.7790, 1.6820],\n",
       "        [0.2458, 0.7442, 0.7297,  ..., 0.6555, 1.4635, 0.4917],\n",
       "        [0.7267, 0.2733, 0.4580,  ..., 0.4321, 0.6894, 1.4175]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dout =  out_exp.grad.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_b_grad, in2_b_grad, w_b_grad = bwd_uvu(in1_b, in2_b, dL_dout , weight_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_bwd_dF_dOut = torch.cat([bwd_uvu.uvuv_result_list[i].reshape(batch_size,-1) for i in bwd_uvu.sort2path],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3880, 0.3156, 0.1872,  ..., 0.7488, 0.4440, 0.3124],\n",
       "        [0.1694, 0.1916, 0.3869,  ..., 0.2643, 0.5337, 0.4475],\n",
       "        [0.0095, 0.0309, 0.0177,  ..., 0.3095, 0.1770, 0.4630],\n",
       "        ...,\n",
       "        [0.3021, 0.5303, 0.0098,  ..., 0.9175, 0.0170, 0.1353],\n",
       "        [0.0589, 0.0205, 0.1172,  ..., 0.0187, 0.1065, 0.1152],\n",
       "        [0.1767, 0.8879, 0.3235,  ..., 0.6278, 0.2288, 0.2125]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwd_bwd_dF_dOut.reshape(batch_size,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3880, 0.5023, 0.0242,  ..., 0.4323, 0.2564, 0.1804],\n",
       "        [0.1694, 0.2809, 0.1469,  ..., 0.1526, 0.3082, 0.2583],\n",
       "        [0.0095, 0.0575, 0.0885,  ..., 0.1787, 0.1022, 0.2673],\n",
       "        ...,\n",
       "        [0.3021, 0.0585, 0.2606,  ..., 0.5297, 0.0098, 0.0781],\n",
       "        [0.0589, 0.2111, 0.2072,  ..., 0.0108, 0.0615, 0.0665],\n",
       "        [0.1767, 0.0599, 0.1110,  ..., 0.3625, 0.1321, 0.1227]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5060, 0.7949, 0.5167,  ..., 0.8254, 0.7008, 0.6429],\n",
       "        [0.6071, 0.6771, 0.5564,  ..., 0.5421, 0.5847, 0.5711],\n",
       "        [0.5028, 0.5178, 0.5271,  ..., 0.6647, 0.5951, 0.7419],\n",
       "        ...,\n",
       "        [0.6042, 0.5466, 0.5999,  ..., 0.7856, 0.5055, 0.5441],\n",
       "        [0.5322, 0.6620, 0.5791,  ..., 0.5134, 0.5761, 0.5823],\n",
       "        [0.6344, 0.5033, 0.5818,  ..., 0.7029, 0.5754, 0.5701]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2805, 1.5944, 0.9703,  ..., 1.8918, 1.3451, 1.1090],\n",
       "        [0.7541, 1.0335, 0.4534,  ..., 0.4600, 0.7867, 0.6821],\n",
       "        [0.0735, 0.2649, 0.3865,  ..., 1.0998, 0.8784, 1.3561],\n",
       "        ...,\n",
       "        [0.6070, 0.6866, 0.5809,  ..., 1.7051, 0.4276, 0.5955],\n",
       "        [0.3539, 1.1995, 0.6624,  ..., 0.2852, 0.6560, 0.6927],\n",
       "        [1.1623, 0.0623, 0.7820,  ..., 1.2202, 0.6200, 0.5955]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dF_dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1_b_grad, in2_b_grad, w_b_grad = bwd_uvu(in1_b, in2_b, dL_dout , weight_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(887212.1250, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in1_c_grad, in2_c_grad, w_c_grad = bwd_uvu(in1_c, in2_c, dE_dout , weight_c)\n",
    "f_in1_gelu = torch.nn.functional.gelu(in1_c_grad)\n",
    "f_in2_gelu = torch.nn.functional.gelu(in2_c_grad)\n",
    "f_weight_gelu = torch.nn.functional.gelu(w_c_grad)\n",
    "\n",
    "fake_loss_f = f_in1_gelu.sum() + f_in2_gelu.sum() + f_weight_gelu.sum()\n",
    "print(fake_loss_f)\n",
    "fake_loss_f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7423, 1.6745, 1.3295,  ..., 1.8285, 1.3473, 3.0079],\n",
       "        [1.7320, 1.4259, 1.6523,  ..., 0.8184, 1.3094, 1.6805],\n",
       "        [1.4283, 1.2062, 1.2244,  ..., 1.2111, 0.9035, 2.0908],\n",
       "        ...,\n",
       "        [1.0257, 1.6077, 0.9544,  ..., 1.2068, 1.8411, 1.8960],\n",
       "        [1.6774, 1.6838, 2.2131,  ..., 1.5985, 1.0611, 1.5779],\n",
       "        [1.9894, 0.9254, 2.0164,  ..., 0.8554, 1.0441, 1.5951]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1.grad - in1_b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4746, 2.5673, 2.4915,  ..., 2.7547, 1.7052, 4.6607],\n",
       "        [2.3667, 1.6227, 2.3653,  ..., 0.8481, 2.2329, 2.0880],\n",
       "        [1.9892, 1.5336, 1.5013,  ..., 1.5270, 1.1403, 2.6657],\n",
       "        ...,\n",
       "        [1.2998, 2.4359, 1.2184,  ..., 1.7500, 2.7503, 2.5633],\n",
       "        [2.2998, 2.1113, 2.9679,  ..., 2.1193, 1.2690, 2.1742],\n",
       "        [2.4509, 1.1871, 2.7462,  ..., 1.2247, 1.3526, 2.0707]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sptp_bwd.sptp_linear_bwd_v1_shared(in1, in2, weight, dL_dout, \n",
    "                                           mem_dL_din1_2, mem_dL_din2_2,mem_dL_dW_2, mem_debug,\n",
    "                                           t_in1_idxing, t_in1_ival, t_in1_related_path_idx, \n",
    "                                           \n",
    "                                           t_path_array1,t_path_array2,t_per_upath_fiber_start, t_path_weight,t_per_path_weight_pos, \n",
    "                                           t_per_upath_fiber_array,t_unique_cg_val, \n",
    "                                           upath_cnt, 1, l_max*2+1\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.2646, 4.7706, 1.1848,  ..., 0.9037, 1.5067, 1.3808],\n",
      "        [1.3026, 6.7545, 5.2440,  ..., 0.8022, 1.0752, 0.9668],\n",
      "        [1.6280, 3.3079, 4.2651,  ..., 0.6249, 0.3846, 0.5027],\n",
      "        ...,\n",
      "        [5.1874, 1.7463, 1.9636,  ..., 0.6805, 0.9507, 0.5868],\n",
      "        [1.1540, 2.9928, 1.3317,  ..., 0.5994, 0.5564, 0.8041],\n",
      "        [2.4731, 2.5021, 2.0758,  ..., 0.9075, 1.2623, 1.0643]],\n",
      "       device='cuda:0')\n",
      "tensor([[6.5785, 5.9241, 1.4275,  ..., 1.1915, 1.8857, 1.7943],\n",
      "        [1.5393, 8.3717, 6.3028,  ..., 0.8520, 1.2310, 1.1216],\n",
      "        [1.9913, 4.0285, 5.1296,  ..., 0.7297, 0.4196, 0.5860],\n",
      "        ...,\n",
      "        [6.2440, 1.9335, 2.3568,  ..., 0.8560, 1.2979, 0.5248],\n",
      "        [1.3531, 3.7285, 1.6639,  ..., 0.5955, 0.5359, 0.9069],\n",
      "        [2.9506, 2.9886, 2.4973,  ..., 1.1122, 1.5329, 1.2736]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(in1.grad)\n",
    "print(mem_dL_din1 + mem_dL_din1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5027, 3.3319, 5.1228,  ..., 1.7868, 1.8400, 1.8842],\n",
       "        [2.5770, 3.3867, 1.8557,  ..., 1.1394, 0.8838, 1.1472],\n",
       "        [3.1812, 1.8583, 3.4825,  ..., 1.4393, 0.9468, 1.0517],\n",
       "        ...,\n",
       "        [2.5716, 0.9875, 2.1731,  ..., 1.3933, 1.4764, 1.8670],\n",
       "        [1.9476, 2.6564, 2.9469,  ..., 1.0605, 0.7719, 1.0328],\n",
       "        [2.7277, 1.7549, 1.5600,  ..., 2.0296, 1.0919, 1.8511]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = out_exp.grad.detach().clone()\n",
    "new_out.requires_grad = True\n",
    "in1_c_grad, in2_c_grad, w_c_grad = bwd_uvu(in1_c, in2_c, new_out , weight_c)\n",
    "in1_c_grad.retain_grad()\n",
    "in2_c_grad.retain_grad()\n",
    "w_c_grad.retain_grad()\n",
    "\n",
    "\n",
    "f_in1_gelu_c = torch.nn.functional.gelu(in1_c_grad)\n",
    "f_in2_gelu_c = torch.nn.functional.gelu(in2_c_grad)\n",
    "f_weight_gelu_c = torch.nn.functional.gelu(w_c_grad)\n",
    "fake_loss_c = f_in1_gelu_c.sum() + f_in2_gelu_c.sum() + f_weight_gelu_c.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_loss_c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[302.5776, 217.1613, 226.8174, 176.6435],\n",
       "        [453.4338, 234.4316, 198.2770, 158.4990],\n",
       "        [444.6406, 204.6053, 227.6118, 153.1021],\n",
       "        ...,\n",
       "        [335.7768, 134.1655, 155.7681, 230.7632],\n",
       "        [236.0521, 182.1424, 220.3352, 208.7974],\n",
       "        [296.4327, 177.2991, 186.4896, 233.1756]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in2_c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[302.5777, 217.1614, 226.8174, 176.6435],\n",
       "        [453.4338, 234.4316, 198.2770, 158.4990],\n",
       "        [444.6406, 204.6053, 227.6118, 153.1021],\n",
       "        ...,\n",
       "        [335.7768, 134.1655, 155.7681, 230.7632],\n",
       "        [236.0521, 182.1424, 220.3352, 208.7974],\n",
       "        [296.4327, 177.2991, 186.4896, 233.1756]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dl_din2_summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2671, 2.4855, 0.2930,  ..., 0.1088, 0.9660, 0.0597],\n",
       "        [0.3870, 1.8454, 3.6596,  ..., 0.2371, 0.0480, 0.4035],\n",
       "        [0.4839, 1.2464, 1.1771,  ..., 0.3033, 0.1836, 0.0693],\n",
       "        ...,\n",
       "        [1.2005, 0.5062, 0.2942,  ..., 0.1568, 0.0677, 0.1113],\n",
       "        [0.8240, 1.8094, 0.7419,  ..., 0.0719, 0.4216, 0.1555],\n",
       "        [2.0162, 1.8861, 0.6361,  ..., 0.3202, 0.2585, 0.0748]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_dL_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2376, 0.8852, 0.9344, 0.8920],\n",
      "        [1.0128, 0.6753, 0.7480, 0.8928],\n",
      "        [0.5494, 1.3505, 1.2065, 0.7359],\n",
      "        ...,\n",
      "        [3.5244, 2.2287, 2.2657, 2.8133],\n",
      "        [2.8726, 4.8894, 3.0728, 4.4809],\n",
      "        [2.0120, 4.9605, 1.9776, 1.4573]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[0.7867, 0.6858, 0.5769, 0.7672],\n",
      "        [0.4979, 0.4621, 0.5937, 0.8309],\n",
      "        [0.5413, 0.6093, 0.4245, 0.8698],\n",
      "        ...,\n",
      "        [2.1065, 2.5849, 1.6669, 3.0937],\n",
      "        [1.0955, 2.1882, 1.2061, 2.7810],\n",
      "        [2.1567, 2.3180, 2.6381, 2.1826]], device='cuda:0',\n",
      "       grad_fn=<GeneratedBackwardFor_sptp_linear_sptp_linear_bwd_v2_shared_defaultBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(f_in2)\n",
    "print(f_in2_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4324, 4.2339, 3.3840, 1.5389],\n",
       "        [0.9778, 1.3487, 2.5574, 4.8300],\n",
       "        [2.3737, 1.1076, 1.0728, 3.2205],\n",
       "        ...,\n",
       "        [1.2971, 1.2289, 0.8152, 0.4706],\n",
       "        [0.4982, 0.8714, 0.5937, 0.6644],\n",
       "        [1.1654, 0.8855, 1.3845, 0.6539]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9973,  3.6291,  0.4055,  ...,  0.9197,  2.1953,  1.9050],\n",
       "        [ 0.2527,  3.7130,  2.6742,  ...,  0.6850,  0.9896,  0.7729],\n",
       "        [ 1.5004,  4.6002,  4.0388,  ...,  0.4224,  0.8066,  0.7461],\n",
       "        ...,\n",
       "        [ 5.3038,  2.3369,  2.4174,  ...,  0.5876,  1.8144,  0.8881],\n",
       "        [ 0.9836,  1.5320,  0.5223,  ...,  0.0271, -0.0212,  0.0568],\n",
       "        [ 1.9541,  2.1570,  0.9595,  ...,  1.1385,  2.7140,  1.6297]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1_c.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cueq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
