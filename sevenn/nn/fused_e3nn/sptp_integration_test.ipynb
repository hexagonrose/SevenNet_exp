{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home2/lsy/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home2/lsy/.cache/torch_extensions/py311_cu124/sptp_linear/build.ninja...\n",
      "Building extension module sptp_linear...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module sptp_linear...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import e3nn\n",
    "import json\n",
    "import cuequivariance as cue\n",
    "from fused_e3nn import fused_uvu_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_Irreps(mul, i_in):\n",
    "    dd = []\n",
    "    for ori_mul, ir in i_in:\n",
    "        dd.append((ori_mul*mul, (ir.l, ir.p)))\n",
    "    return e3nn.o3.Irreps(dd)\n",
    "def compare(a, b):\n",
    "    isclose = torch.isclose(a, b)\n",
    "    diff_pos = torch.argwhere(isclose == False)\n",
    "    anything_bad = False\n",
    "    for pos in diff_pos:\n",
    "        pos_t = [x for x in pos]\n",
    "        if(abs(a[pos_t] - b[pos_t]) > 1e-6):\n",
    "            anything_bad = True\n",
    "            print(pos)\n",
    "            print(a[pos_t] - b[pos_t] )\n",
    "    if(not anything_bad):\n",
    "        print(\"All Good\")\n",
    "            \n",
    "IR_IN1_IDX = 0\n",
    "IR_IN2_IDX = 1\n",
    "IR_OUT_IDX = 2\n",
    "INST_IDX = 3\n",
    "\n",
    "def load_nequip_config(h, l_max, layer_idx):\n",
    "    filename = f\"/home2/lsy/mdsim/nequip/benchmark_config/4_{h}_{l_max}_p_sc.txt\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        f_in = f.read().split(\"\\n\")\n",
    "\n",
    "    per_layer_dict = dict()\n",
    "    for l_idx, d in enumerate(f_in):\n",
    "        if(d == \"\") : continue\n",
    "        dd = json.loads(d)\n",
    "        per_layer_dict[l_idx] = dd\n",
    "    tp_list = per_layer_dict[layer_idx][\"tp\"]\n",
    "    i_in1 = e3nn.o3.Irreps(tp_list[IR_IN1_IDX])\n",
    "    i_in2 = e3nn.o3.Irreps(tp_list[IR_IN2_IDX])\n",
    "    i_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "    inst_tuple = [tuple(x) for x in tp_list[INST_IDX]]\n",
    "\n",
    "    return i_in1, i_in2, i_out, inst_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nequip_config_e3nn_cueq(h, l_max, layer_idx):\n",
    "    filename = f\"/home2/lsy/mdsim/nequip/benchmark_config/4_{h}_{l_max}_p_sc.txt\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        f_in = f.read().split(\"\\n\")\n",
    "\n",
    "    per_layer_dict = dict()\n",
    "    for l_idx, d in enumerate(f_in):\n",
    "        if(d == \"\") : continue\n",
    "        dd = json.loads(d)\n",
    "        per_layer_dict[l_idx] = dd\n",
    "    tp_list = per_layer_dict[layer_idx][\"tp\"]\n",
    "\n",
    "    ei_in1 = e3nn.o3.Irreps(tp_list[IR_IN1_IDX])\n",
    "    ei_in2 = e3nn.o3.Irreps(tp_list[IR_IN2_IDX])\n",
    "    ei_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "    inst_tuple = [tuple(x) for x in tp_list[INST_IDX]]\n",
    "\n",
    "\n",
    "    # changing mul for each ir.l\n",
    "    new_in1_list = []\n",
    "    new_out_list = []\n",
    "    changed_idx = [[],[]]\n",
    "    # mul_list = {}\n",
    "    mul_list = {0:128, 1:64}\n",
    "\n",
    "    for idx, (mul,ir) in enumerate(ei_in1):\n",
    "        if (ir.l in mul_list):\n",
    "            new_in1_list.append((mul_list[ir.l], ir))\n",
    "            for inst in inst_tuple:\n",
    "                if(idx == inst[0]):\n",
    "                    changed_idx[0].append(inst[2])\n",
    "                    changed_idx[1].append(mul_list[ir.l])\n",
    "        else:\n",
    "            new_in1_list.append((mul, ir))\n",
    "\n",
    "    for idx, (mul,ir) in enumerate(ei_out):\n",
    "        if (idx in changed_idx[0]):\n",
    "            new_out_list.append((changed_idx[1][changed_idx[0].index(idx)], ir))\n",
    "        else:\n",
    "            new_out_list.append((mul, ir))\n",
    "\n",
    "    ei_in1 = e3nn.o3.Irreps(new_in1_list)\n",
    "    ei_out = e3nn.o3.Irreps(new_out_list)\n",
    "\n",
    "    ci_in1 = cue.Irreps(\"O3\", str(ei_in1))\n",
    "    ci_in2 = cue.Irreps(\"O3\", tp_list[IR_IN2_IDX])\n",
    "    ci_out = cue.Irreps(\"O3\", str(ei_out))\n",
    "\n",
    "\n",
    "    return [ei_in1,ei_in2,ei_out,inst_tuple] , [ci_in1,ci_in2,ci_out,inst_tuple]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda_list(*arg, input_dtype = torch.float32):\n",
    "    return_list = []\n",
    "    for item in arg:\n",
    "        if(type(item) == torch.Tensor):\n",
    "            return_list.append(item.to(device=\"cuda\"))\n",
    "        else:\n",
    "            return_list.append(torch.tensor(item,device=\"cuda\", dtype=input_dtype))\n",
    "    return return_list\n",
    "\n",
    "def to_cuda_dict(strname_list, *arg):\n",
    "    return_dict = {}\n",
    "    for item,name in zip(arg,strname_list):\n",
    "        if(type(item) == torch.Tensor):\n",
    "            return_dict[name] = item.to(\"cuda\")\n",
    "        else:\n",
    "            return_dict[name] = torch.tensor(item,device=\"cuda\")\n",
    "    return return_dict\n",
    "\n",
    "def cumsum_list(s, np1 = True):\n",
    "    new_s = []\n",
    "    current = 0\n",
    "    for e in s:\n",
    "        new_s.append(current)\n",
    "        current += e\n",
    "    if(np1):\n",
    "        new_s.append(current)\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home2/lsy/miniconda3/envs/cueq/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "h = 32\n",
    "l_max = 2\n",
    "layer_idx = 3\n",
    "batch_size = 4096\n",
    "# i_in1, i_in2, i_out, inst_tuple = load_nequip_config(h,l_max,layer_idx)\n",
    "# i_in2 = mul_Irreps(3, i_in2)\n",
    "e3nn_config, cueq_config = load_nequip_config_e3nn_cueq(h,l_max,layer_idx)\n",
    "i_in1, i_in2, i_out, inst_tuple = e3nn_config\n",
    "\n",
    "# not really needed for v=1 \n",
    "uvuv_tp = e3nn.o3.FullTensorProduct(i_in1,i_in2, filter_ir_out=i_out)\n",
    "uvuv_i_out = uvuv_tp.irreps_out\n",
    "\n",
    "# split_size = []\n",
    "# reshape_size = []\n",
    "# for inst in uvuv_tp.instructions:\n",
    "#     split_size.append(uvuv_i_out[inst.i_out].dim)\n",
    "#     reshape_size.append([inst.path_shape[0],inst.path_shape[1],uvuv_i_out[inst.i_out][1].dim])\n",
    "# weight_mul = e3nn.o3.experimental.FullTensorProduct_uvu_weight_only(i_in1, i_in2, split_size, reshape_size, filter_ir_out=i_out, irrep_normalization=None, regroup_output=False)\n",
    "# uvw\n",
    "# i_out = e3nn.o3.Irreps(tp_list[IR_OUT_IDX])\n",
    "# tp = e3nn.o3.FullyConnectedTensorProduct(i_in1,i_in2,i_out,shared_weights=False, internal_weights=False)\n",
    "\n",
    "# # uvu\n",
    "tp = e3nn.o3.TensorProduct(i_in1,i_in2,i_out,inst_tuple,shared_weights=False, internal_weights=False)\n",
    "\n",
    "fused_tp = fused_uvu_TP(l_max, i_in1,i_in2,i_out,inst_tuple,per_block_batch=4, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "in1 = torch.rand(batch_size, i_in1.dim, device=\"cuda\", requires_grad=True)\n",
    "in2 = torch.rand(batch_size, i_in2.dim, device=\"cuda\", requires_grad=True)\n",
    "# weight = torch.ones(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "weight = torch.rand(batch_size,tp.weight_numel, device=\"cuda\", requires_grad=True)\n",
    "\n",
    "in1_c = in1.detach().clone()\n",
    "in2_c = in2.detach().clone()\n",
    "weight_c = weight.detach().clone()\n",
    "in1_c.requires_grad =True\n",
    "in2_c.requires_grad =True\n",
    "weight_c.requires_grad =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = tp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_exp = tp(in1,in2,weight)\n",
    "y = torch.nn.functional.gelu(out_exp).sum()\n",
    "f_in1, f_in2, f_weight = torch.autograd.grad(y, [in1,in2,weight], create_graph=True)\n",
    "# out_exp.grad = None\n",
    "\n",
    "f_in1_gelu = torch.nn.functional.gelu(f_in1)\n",
    "f_in2_gelu = torch.nn.functional.gelu(f_in2)\n",
    "f_weight_gelu = torch.nn.functional.gelu(f_weight)\n",
    "fake_loss = f_in1_gelu.sum() + f_in2_gelu.sum() + f_weight_gelu.sum()\n",
    "fake_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ours = fused_tp(in1_c,in2_c,weight_c)\n",
    "y_ours = torch.nn.functional.gelu(out_ours).sum()\n",
    "f_in1_c, f_in2_c, f_weight_c = torch.autograd.grad(y_ours, [in1_c,in2_c,weight_c], create_graph=True)\n",
    "# out_ours.grad = None\n",
    "\n",
    "f_in1_gelu_c = torch.nn.functional.gelu(f_in1_c)\n",
    "f_in2_gelu_c = torch.nn.functional.gelu(f_in2_c)\n",
    "f_weight_gelu_c = torch.nn.functional.gelu(f_weight_c)\n",
    "fake_loss_c = f_in1_gelu_c.sum() + f_in2_gelu_c.sum() + f_weight_gelu_c.sum()\n",
    "fake_loss_c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5754, 0.5542, 0.2607,  ..., 3.4840, 4.6964, 3.2508],\n",
       "        [1.3899, 0.8044, 0.6463,  ..., 0.3992, 0.6009, 1.1369],\n",
       "        [1.8709, 1.4929, 0.9988,  ..., 3.0580, 2.0395, 1.0270],\n",
       "        ...,\n",
       "        [1.1788, 1.4602, 1.9032,  ..., 4.9027, 1.3650, 2.8675],\n",
       "        [0.6461, 0.3524, 1.3731,  ..., 0.3250, 2.1968, 1.5510],\n",
       "        [0.4353, 0.7938, 0.8230,  ..., 2.5534, 1.3907, 3.4741]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5754, 0.5542, 0.2607,  ..., 3.4840, 4.6964, 3.2508],\n",
       "        [1.3899, 0.8044, 0.6463,  ..., 0.3992, 0.6009, 1.1369],\n",
       "        [1.8709, 1.4929, 0.9988,  ..., 3.0580, 2.0395, 1.0270],\n",
       "        ...,\n",
       "        [1.1788, 1.4602, 1.9032,  ..., 4.9027, 1.3650, 2.8675],\n",
       "        [0.6461, 0.3524, 1.3731,  ..., 0.3250, 2.1968, 1.5510],\n",
       "        [0.4353, 0.7938, 0.8230,  ..., 2.5534, 1.3907, 3.4741]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cueq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
